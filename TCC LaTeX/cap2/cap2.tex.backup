
\chapter{\nohyphens{Álgebra Linear para Computação Quântica}}\label{cap2_AL}


 Neste capítulo, um resumo de Álgebra Linear voltado para Computação Quântica é apresentado. A teoria é apresentada usando-se a \emph{notação de Dirac}, ou, \emph{notação de braket}, uma notação utilizada largamente em Mecânica Quântica e que será necessária para o restante do trabalho. Essa notação é conveniente para se fazer contas e faz com que as diversas operações possíveis se encaixem naturalmente. 
 
 Na Álgebra Linear abstrata, pode-se considerar espaços vetoriais sobre diversos corpos (ou escalares), que são estruturas algébricas com propriedades semelhantes aos números reais e complexos. Os espaços vetoriais podem ter dimensão finita ou infinita. 

 Na Computação Quântica o interesse é voltado a espaços vetoriais de dimensão finita sobre o corpo dos números complexos. Isso permite a identificação do espaço com as $n$-uplas de números complexos, o que simplifica grandemente a teoria. Os resultados resumidos neste capítulo estão situados nesse contexto. 
 
 A principal referência para esse capítulo é \cite{book:qcqi_nc}. Outra referência muito útil é \cite{book:al_elon}, que possui um capítulo voltado a espaços vetoriais complexos. Livros texto clássicos de Álgebra Linear, como \cite{book:al_steinbruch} também são úteis. Embora tenham ênfase em espaços vetoriais reais, a maioria das definições, resultados e demonstrações se transporta integralmente para os espaços vetoriais complexos. 
 
 Será considerado um pré-requisito a este texto um curso de Álgebra Linear ao nível de graduação abordando-se os seguintes itens: espaços vetoriais, base e dimensão, transformações lineares, autovalores e autovetores. Por ter um caráter de resumo, os resultados apresentados, via de regra, não são acompanhados de suas demonstrações, as quais podem ser encontradas nos livros mencionados no parágrafo anterior.
 
 
\begin{section}{\nohyphens{Espaço Vetorial, Base e Dimensão}}

\begin{subsection}{Espaço Vetorial}
 O conjunto das $n$-uplas $(z_0, \ldots, z_{n-1})$ de números complexos com a soma e o produto por escalar definidos entrada a entrada é um \emph{Espaço Vetorial Complexo} e é denotado por $\mathbb{C}^n$. É conveniente representar esses elementos por vetores coluna. Tem-se então:
 \[ \mqty[z_0 \\ z_{1} \\ \vdots \\ z_{n-1}] + \mqty[w_0 \\ w_{1} \\ \vdots \\ w_{n-1}] = \mqty[z_0 + w_0\\ z_{1} + w_1 \\ \vdots \\ z_{n-1} + w_{n-1}]  \ \ \ \text{ e } \ \ \   z \cdot \mqty[z_0 \\ z_{1} \\ \vdots \\ z_{n-1}] = \mqty[z \cdot z_0 \\ z\cdot z_{1} \\ \vdots \\ z \cdot z_{n-1}] \ .\]
 
 Em Mecânica Quântica, os vetores de $\mathbb{C}^n$ costumam ser usados na \emph{notação de Dirac}, ou \emph{notação de braket}:
 \[ \ket{\psi} = (z_0 , z_{1} , \ldots , z_{n-1} ) =  \mqty[z_0 \\ z_{1} \\ \vdots \\ z_{n-1}]   \ \ . \]
 Um vetor $\ket{\psi}$ é chamado \emph{ket} (em contraponto com $\bra{\psi}$, que será definido posteriormente, e será chamado  \emph{bra}). 
 
 Os vetores se comportam de maneira semelhante aos números no que diz respeito à soma e subtração. Em particular, a soma comuta $\ket{\phi} + \ket{\psi} = \ket{\psi} + \ket{\phi}$, há um vetor nulo, denotado por $0$ ou $\ket{\varnothing} = (0, \ldots , 0)$ de tal forma que $\ket{\psi} + 0 = \ket{\psi}$ e, ainda, vale $-\ket{\psi} = (-z_0, \dots , -z_{n-1}) = -1 \cdot \ket{\psi}$. O produto por escalar também se comporta de maneira semelhante ao produto numérico, e valem as propriedades distributivas $z(\ket{\phi} + \ket{\psi}) = z \ket{\phi} + z \ket{\psi}$ e $(z+w)\ket{\psi} = z\ket{\psi} + w \ket{\psi}$, por exemplo.
 
 \begin{example}[Espaço de estados de 1 qubit]
  O conjunto  
  \[ \mathbb{C}^2 = \left\{ \ket{\psi} = \mqty[ a \\ b ] : a,b \in \mathbb{C}\right\} \]
  é um espaço vetorial com soma e produto por escalar dados por
  \[ \mqty[ a_1 \\ b_1 ] + \mqty[ a_2 \\ b_2 ] = \mqty[ a_1 + a_2 \\ b_1 + b_2 ] \]
  \[ z \cdot \mqty[ a \\ b ] = \mqty[ za \\ zb ] \ . \]
  Esse espaço vetorial será largamente utilizado nos capítulos seguintes e descreve o espaço de estados de 1 \emph{qubit}, o análogo do bit clássico. 
 \end{example}

 \end{subsection}
 
 \begin{subsection}{Base e Dimensão}
 Uma base para o espaço vetorial $\mathbb{C}^n$ é um conjunto de vetores \emph{linearmente independentes} (LI) e que \emph{geram o espaço}.  Demonstra-se que todas as bases de um espaço vetorial têm o mesmo número de elementos, e define-se a \emph{dimensão} do espaço vetorial pelo número de elementos de uma base. 
 
 O espaço vetorial $\mathbb{C}^n$ tem dimensão $n$, isto é, todas as suas bases têm $n$ vetores. Uma base muito útil é a chamada \emph{base computacional}, ou \emph{base canônica}\footnote{O adjetivo ``canônico'', na Matemática, tem um sentido de ``padrão'', como na expressão ``configuração padrão''.}:
 \[ \ket{0} = \mqty[1 \\ 0 \\ \vdots \\ 0] \ \ , \ \ \ket{1} = \mqty[0 \\ 1 \\ \vdots \\ 0 ]  \ \ , \ \ \cdots \ \ , \ \ \ket{n-1}^= \mqty[0 \\ 0 \\ \vdots \\ 1]  \ \ . \]
 
 Na base computacional, um vetor $\ket{\psi} = (z_0 , z_{1} , \ldots , z_{n-1} )$ é escrito como
  \[ \ket{\psi} = z_0 \ket{0} + z_1 \ket{1} + \ldots + z_{n-1} \ket{n-1} \ \ . \]
 
  Numa base qualquer $\beta = \big\{\ket{b_0}, \ldots, \ket{b_{n-1}} \big\} $, qualquer vetor $\psi$ pode ser escrito como combinação linear dos vetores dessa base. Os coeficientes da combinação linear, colocados em um vetor coluna, representam o vetor $\psi$ escrito na base $\beta$, conforme o esquema abaixo:
  \[ \ket{\psi} = a_0 \ket{b_0} + \ldots + a_{n-1} \ket{b_{n-1}} \ \  \Leftrightarrow \ \ \big[ \ket{\psi} \big]_\beta = \mqty[ a_0 \\ \vdots \\ a_{n-1} ]_\beta \ .    \]
  O subscrito $\beta$ pode ser omitido se não houver risco de confusão. Normalmente omite-se esse subscrito quando se trata da base computacional.
  \end{subsection}
  
  \begin{example}[Bases para 1 qubit]\label{cap2:bases_1qubit}
 Há especial interesse no espaço $\mathbb{C}^2$. Este espaço modela um \emph{qubit} --- o análogo quântico do bit --- a ser discutido em mais detalhes posteriormente. O espaço $\mathbb{C}^2$ tem dimensão $2$ e admite, entre outras, as seguintes bases:
 \begin{eqnarray*}
  \mathcal{I} = \mathcal{Z} &=& \big\{ \ket{0} \  , \  \ket{1} \big\} \\
  \mathcal{X} &=& \left\{ \ket{+} = \tfrac{1}{\sqrt{2}}( \ket{0} + \ket{1}) \ , \  \ket{-} = \tfrac{1}{\sqrt{2}}( \ket{0} - \ket{1})  \right\} \\
  \mathcal{Y} &=& \left\{ \ket{+i} = \tfrac{1}{\sqrt{2}}( \ket{0} + i\ket{1}) , \ket{-i} = \tfrac{1}{\sqrt{2}}( \ket{0} -i \ket{1}) \right\} \ \ .
 \end{eqnarray*}
 Essa notação para as bases será justificada \emph{a posteriori}.
 \end{example}
 
 \begin{example}[$\mathcal{X}$ é base para 1 qubit]
  Para mostrar que 
  \[ \mathcal{X} = \left\{ \ket{+} = \tfrac{1}{\sqrt{2}}( \ket{0} + \ket{1}) \ , \  \ket{-} = \tfrac{1}{\sqrt{2}}( \ket{0} - \ket{1})  \right\} \]
  é base do espaço de estados de 1 qubit, deve-se mostrar que os vetores são LI (Linearmente Independentes) e geram o espaço $\mathbb{C}^2$.
  \\ \\
  \noindent \textbf{$\mathcal{X}$ é LI:} Considere a combinação linear nula:
  \begin{eqnarray*} 
  a_0 \ket{+} + a_1 \ket{-} = 0 \\
  a_0 \frac{1}{\sqrt{2}}( \ket{0} + \ket{1}) + a_1 \frac{1}{\sqrt{2}}( \ket{0} - \ket{1}) = 0 \\
  \frac{a_0 + a_1}{\sqrt{2}} \ket{0} + \frac{a_0 - a_1}{\sqrt{2}} \ket{1} = 0
  \end{eqnarray*}
  Tem-se que
  \[ \begin{cases}
      \frac{a_0 + a_1}{\sqrt{2}} = 0 \\
      \frac{a_0 - a_1}{\sqrt{2}} = 0
     \end{cases}
     \implies
     \begin{cases}
      a_0 + a_1 = 0 \\
      a_0 - a_1 = 0 
     \end{cases} 
     \implies
     \begin{cases}
      a_0 = 0 \\
      a_1 = 0 \ .
     \end{cases}
     \]
     
  Portanto os coeficientes da combinação linear nula devem ser todos nulos, e isso significa que os vetores $\ket{+}$ e $\ket{-}$ são LI.
  \\ \\
  \noindent \textbf{$\mathcal{X}$ gera o espaço:} Seja $\ket{\psi} = z_0 \ket{0} + z_1 \ket{1}$ um vetor qualquer de $\mathbb{C}^2$. Tenta-se escrever $\ket{\psi}$ como combinação linear de $\ket{+}$ e $\ket{-}$. Se for possível, esses vetores geram o espaço.
  \begin{eqnarray*}
    z_0 \ket{0} + z_1 \ket{1} 
    &=& a_0 \ket{+} + a_1 \ket{-} \\
    &=& a_0 \frac{1}{\sqrt{2}}( \ket{0} + \ket{1}) + a_1 \frac{1}{\sqrt{2}}( \ket{0} - \ket{1}) \\
    &=&  \frac{a_0 + a_1}{\sqrt{2}} \ket{0} + \frac{a_0 - a_1}{\sqrt{2}} \ket{1}
  \end{eqnarray*}
  
 Assim,
 \[ \begin{cases}
     \frac{a_0 + a_1}{\sqrt{2}} = z_0 \\ 
     \frac{a_0 - a_1}{\sqrt{2}} = z_1
    \end{cases} 
    \implies
    \begin{cases}
     a_0 = \frac{z_0 + z_1}{\sqrt{2}} \\
     a_1 = \frac{z_0 - z_1}{\sqrt{2}}
    \end{cases}
    \]

    Dessa forma, $\mathcal{X}$ gera o espaço $\mathbb{C}^2$ e é base desse espaço.
    \\ \\
    \noindent \textbf{Comentário:} Quando se sabe previamente que a dimensão do espaço é $n$ e se a lista de vetores candidatos a base tem $n$ elementos, então as condições de ser LI e gerar o espaço são equivalentes. Em consequência, basta verificar uma das condições para mostrar que os vetores formam uma base. Por exemplo, se sabemos que a dimensão de $\mathbb{C}^2$ é $\dim \mathbb{C}^2 = 2$, e temos que $\mathcal{X}$ tem dois elementos, então bastaria verificar uma das duas condições: $\mathcal{X}$ é LI ou $ \mathcal{X}$ gera o espaço. 
 \end{example}

 
 \begin{subsection}{\nohyphens{Matriz de Mudança de Base}}\label{cap2:matriz_de_mudança_de_base}
  Por vezes é conveniente expressar um vetor em outra base que não a base computacional. Essa mudança de base pode trazer novo \emph{insight} em algumas situações, bem como tornar os cálculos mais simples e factíveis. 
  
  A matriz de mudança de base de $\beta_{\text{old}} = \big\{\ket{u_0}, \ldots, \ket{u_{n-1}} \big\} $ para $\beta_{\text{new}} = \big\{ \ket{v_0}, \ldots, \ket{v_{n-1}} \big\} $ é dada por
 \[ [I]^{\beta_{\text{old}}}_{\beta_{\text{new}} } = \left[ \begin{matrix} | & & | \\ \big[\ket{v_0}\big]_{\beta_{\text{old}}} & \cdots & \big[\ket{v_{n-1}}\big]_{\beta_{\text{old}}} \\  | & & | \end{matrix} \right] \ . \]
 Isto é, para montar a matriz de mudança de base, os vetores da base nova devem ser escritos como combinação linear dos vetores da base antiga, obtendo vetores coluna. Esses vetores serão as colunas da matriz de mudança de base. Dessa forma, tem-se
 \[ [v]_{\beta_{\text{new}}} = [I]^{\beta_{\text{old}}}_{\beta_{\text{new}} } [v]_{\beta_{\text{old}}} \ . \]
 A matriz de mudança de base admite matriz inversa, que corresponde à mudança de base da base nova de volta para a base antiga: 
 \[ [I]^{\beta_{\text{old}}}_{\beta_{\text{new}} }\phantom{\big{|}}^{-1} = [I]^{\beta_{\text{new}}}_{\beta_{\text{old}} } \ . \] 
 
 \begin{example}\label{cap2:ex_matrix_mudança_de_base}
  Considere as bases $\mathcal{I}$ e $\mathcal{X}$ apresentadas no exemplo \ref{cap2:bases_1qubit}. A matriz de mudança de base da base computacional $\mathcal{I}$ para a base $\mathcal{X}$ é obtida escrevendo os vetores da base nova ($\mathcal{X}$) como combinação linear dos vetores na base antiga ($\mathcal{I}$):
  \begin{eqnarray*}
   \ket{+} 
   &=& \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt{2}}  \ket{1}  = \mqty[ \frac{1}{\sqrt{2}} \vspace{4pt} \\ \frac{1}{\sqrt{2}} ]_{\mathcal{I}}\\
   \ket{-}
   &=& \frac{1}{\sqrt{2}} \ket{0} - \frac{1}{\sqrt{2}} \ket{1} = \mqty[ \phantom{-} \frac{1}{\sqrt{2}} \vspace{4pt} \\  - \frac{1}{\sqrt{2}} ]_{\mathcal{I}}
  \end{eqnarray*}
  Coloca-se as os vetores coluna na ordem em que aparecem na lista:
  \[ [I]^\mathcal{I}_\mathcal{X} = \mqty[  \mqty[ \frac{1}{\sqrt{2}} \vspace{4pt} \\ \frac{1}{\sqrt{2}} ]_{\mathcal{I}} & \mqty[ \phantom{-} \frac{1}{\sqrt{2}} \vspace{4pt} \\  - \frac{1}{\sqrt{2}} ]_{\mathcal{I}} ] = \frac{1}{\sqrt{2}} \mqty[1 & \phantom{-}1 \\ 1 & -1  ] \ . \] 
  Essa matriz é conhecida em Computação Quântica como \emph{matriz de Hadamard}, e costuma ser denotada por $H$. Portanto
  \[ H = [I]^\mathcal{I}_\mathcal{X} = \frac{1}{\sqrt{2}} \mqty[1 & \phantom{-}1 \\ 1 & -1  ] \ . \]
  Também vale que a matriz de mudança de base de $\mathcal{X}$ para $\mathcal{I}$ é $H$, visto que 
  \[ HH = I  \implies H^{-1} = H \implies [I]_\mathcal{I}^\mathcal{X} = \big([I]^\mathcal{I}_\mathcal{X} \big)^{-1} = H^{-1} = H \ . \]
  Então: 
  \[ \begin{array}{l}
   H\ket{0} = \ket{+} \\
   H\ket{1} = \ket{-}
  \end{array} 
  \quad \quad
  \begin{array}{l}
  H\ket{+} = \ket{0} \\
  H\ket{-} = \ket{1}
 \end{array} \ . \]
 \end{example}

 \end{subsection}

 
\end{section}

\begin{section}{\nohyphens{Produto Interno, Norma e Produto Exterior}}

 \begin{subsection}{Produto Interno}
 O espaço vetorial $\mathbb{C}^n$ admite o seguinte \emph{produto interno}:
 \[ \big( \ket{\phi} , \ket{\psi} \big) = \braket{\phi}{\psi} = \mqty[w_0^{\ *} & w_1^{\ *} & \cdots & w_{n-1}^{\ *}] \cdot \mqty[z_0 \\ z_1 \\ \vdots \\ z_{n-1}] = \sum_{k=0}^{n-1} w_k^{\ *} z_k \ \ \ , \]
 para $\ket{\phi} = \smqty[w_0 \\ \vdots \\ w_{n-1}]$ e $\ket{\psi} = \smqty[z_0 \\ \vdots \\ z_{n-1}]$. Pode-se considerar o símbolo $\bra{\phi}$ de maneira independente, definindo-o como:
 \[ \bra{\phi} = \ket{\phi}^\dag = \mqty[w_0 \\ \vdots \\ w_{n-1}]^\dag =  \mqty[w_0^{\ *} & w_1^{\ *} & \cdots & w_{n-1}^{\ *}] \ \ , \]
 em que o símbolo $\dag$ denota transposição e conjugação do vetor. Essa notação também será justificada posteriormente. 
 
 A operação definida acima satisfaz as propriedades que definem um produto interno de maneira geral:
 \begin{itemize}
  \item[(PI1)]\label{pi1} Linearidade no segundo argumento: \[ \big( \ket{\phi} , z_1\ket{\psi_1} + z_2 \ket{\psi_2} \big) = z_1 \big( \ket{\phi} , \ket{\psi_1} \big) + z_2 \big( \ket{\phi} , \ket{\psi_2} \big) \]
  \item[(PI2)]\label{pi2} Antilinearidade no primeiro argumento: \[ \big( z_1 \ket{\phi_1} + z_2 \ket{\phi_2} , \ket{\psi} \big) = z_1^{\ *} \big( \ket{\phi_1} , \ket{\psi} \big) + z_2^{\ *} \big( \ket{\phi_2} , \ket{\psi} \big) \]
  \item[(PI3)]\label{pi3} Simetria hermitiana: 
  \[ \big( \ket{\phi} , \ket{\psi} \big)^* = \big( \ket{\psi} , \ket{\phi} \big) \]
  \item[(PI4)]\label{pi4} Positividade:
  \[ \big( \ket{\phi} , \ket{\phi} \big) \geq 0 \ \ \  \text{e} \ \ \ \big( \ket{\phi} , \ket{\phi} \big) = 0  \Leftrightarrow \ket{\psi} = 0\]
 \end{itemize}
  A propriedade (PI2) decorre de (PI1) e de (PI3), mas foi incluída na lista por completeza. Por causa das propriedades (PI1) e (PI2), o produto interno é dito ser \emph{sesquilinear}\footnote{O prefixo \emph{sesqui} significa ``um e meio''.}.
  
  O espaço vetorial $\mathbb{C}^n$ é, pois, dito ser um \emph{espaço vetorial com produto interno}, ou ainda, um \emph{espaço de Hilbert}\footnote{Um espaço de Hilbert é definido como sendo um espaço vetorial com produto interno e com uma propriedade adicional chamada \emph{completude}. Essa propriedade é automática para espaços de dimensão finita.}.
  
  \begin{example}
   O produto interno dos vetores
   \[ \ket{\phi} = \frac{i}{2} \ket{0} + \frac{\sqrt{3}}{2} \ket{1} = \mqty[ \frac{i}{2} \vspace{4pt} \\ \frac{\sqrt{3}}{2}]  \ \text{ e } \  \ket{\psi} = \frac{1}{\sqrt{2}} \ket{0} + \frac{-i}{\sqrt{2}} \ket{1} = \mqty[\frac{1}{\sqrt{2}} \vspace{4pt} \\ \frac{-i}{\sqrt{2}} ] \]
   é dado por:
   \begin{eqnarray*} 
    \braket{\phi}{\psi} 
    &=& \left( \frac{-i}{2} \bra{0} + \frac{\sqrt{3}}{2} \bra{1} \right) \left( \frac{1}{\sqrt{2}} \ket{0} + \frac{-i}{\sqrt{2}} \ket{1} \right)  \\
    &=&
    \frac{-i}{2\sqrt{2}} \braket{0}{0} + \frac{-i^2}{2\sqrt{2}} \braket{0}{1} + \frac{\sqrt{3}}{2\sqrt{2}} \braket{1}{0} + \frac{-i\sqrt{3}}{2\sqrt{2}} \braket{0}{0} \\
    &=& 
     \frac{-i}{2\sqrt{2}} + 0 + 0 + \frac{-i\sqrt{3}}{2\sqrt{2}} = -i\frac{1+\sqrt{3}}{2\sqrt{2}} \ .
   \end{eqnarray*}

   Esse produto interno também pode ser calculado de maneira matricial:
   \begin{eqnarray*} 
    \braket{\phi}{\psi} 
    &=&  \mqty[ \frac{i}{2} \vspace{4pt} \\ \frac{\sqrt{3}}{2}]^\dag \cdot \mqty[\frac{1}{\sqrt{2}} \vspace{4pt} \\ \frac{-i}{\sqrt{2}} ] \\
    &=&   \mqty[ \frac{i}{2}^{*}  & \frac{\sqrt{3}}{2}^{*}] \cdot \mqty[\frac{1}{\sqrt{2}} \vspace{4pt} \\ \frac{-i}{\sqrt{2}} ]   \\
    &=& \mqty[ \frac{-i}{2}  & \frac{\sqrt{3}}{2}] \cdot \mqty[\frac{1}{\sqrt{2}} \vspace{4pt} \\ \frac{-i}{\sqrt{2}} ]   \\
    &=&  \frac{-i}{2\sqrt{2}} + \frac{-i\sqrt{3}}{2\sqrt{2}} 
     \end{eqnarray*}
     
  \end{example}

  \end{subsection}
  
  \begin{subsection}{Norma}
  A \emph{norma}, ou \emph{tamanho}, de um vetor é definida por
  \[ \norm{\ket{\psi}} = \sqrt{\braket{\psi}} \geq 0 \ \ , \]
  operação bem definida pois $\braket{\psi}$ é real e não-negativo. Por consequência da propriedade (P4), tem-se $\norm{\ket{\psi}} = 0 \Leftrightarrow \ket{\psi} = 0$. Um vetor \emph{normalizado} é um vetor de tamanho unitário, e a operação de \emph{normalização} consiste em multiplicar o vetor $\ket{\psi}$ por $\frac{1}{\norm{\ket{\psi}}}$ para que o vetor resultante $\frac{\ket{\psi}}{\norm{\ket{\psi}}}$ tenha norma 1. 
  
  \begin{example}\label{cap2:ex_base_comp_norma_1}
   Os vetores $\ket{0}$, $\ket{1}$, $\ket{+}$ e $\ket{-}$ têm norma 1. 
  \end{example}

  
  \begin{example}
   A norma do vetor $\ket{\psi} = \frac{i}{2} \ket{0} + \frac{\sqrt{3}}{2} \ket{1}$ é
   \[ \norm{\ket{\psi}} =  \sqrt{\abs{\frac{i}{2}}^2 + \abs{\frac{\sqrt{3}}{2}}^2}  = \sqrt{\frac{1}{4} + \frac{3}{4}} = 1 \ . \] 
  \end{example}

  \end{subsection}
  
  \begin{subsection}{Ortogonalidade}
   De forma análoga com o que se passa com vetores no $\mathbb{R}^3$, dois vetores $\ket{\phi}$ e $\ket{\psi}$ são ditos \emph{ortogonais} se o produto interno entre eles é nulo. Em símbolos, $\ket{\phi} \perp \ket{\psi} \ \Leftrightarrow \ \braket{\phi}{\psi} = 0$. 
   
   \begin{example}\label{cap2:ex_ortogonalidade_base_comp}
    Os vetores $\ket{0}$ e $\ket{1}$ são ortogonais: 
    \[ \braket{0}{1} = \mqty[1 & 0 ] \cdot \mqty[0 \\ 1] = 0 \ . \]
   \end{example}
   
   \begin{example}\label{cap2:ex_ortogonalidade_base_x}
    Os vetores $\ket{+} =  \frac{1}{\sqrt{2}} \ket{0} +  \frac{1}{\sqrt{2}} \ket{1}$ e $\ket{-} =  \frac{1}{\sqrt{2}} \ket{0} -  \frac{1}{\sqrt{2}} \ket{1}$ são ortogonais:
    \begin{eqnarray*}
     \braket{+}{-} 
     &=& \left( \frac{1}{\sqrt{2}} \bra{0} +  \frac{1}{\sqrt{2}} \bra{1} \right) \left( \frac{1}{\sqrt{2}} \ket{0} - \frac{1}{\sqrt{2}} \ket{1} \right) \\
     &=& \frac{1}{2} \braket{0}{0} - \frac{1}{2} \braket{0}{1} + \frac{1}{2} \braket{1}{0}  - \frac{1}{2} \braket{1}{1} \\
     &=& \frac{1}{2} - 0 + 0 - \frac{1}{2} = 0 \ .
    \end{eqnarray*}

   \end{example}


  \end{subsection}

  \begin{subsection}{Base Ortonormal}\label{cap2:base_ortonormal}
   Uma base em que todos os vetores são ortogonais dois a dois e têm tamanho 1 é dita ser \emph{base ortonormal}. Se $\beta = \{\ket{b_0}, \ldots, \ket{b_{n-1}} \} $ é uma tal base, vale a chamada \emph{relação de ortogonalidade}
   \[ \braket{b_k}{b_l} = \delta_{k,l} = \left\lbrace \begin{matrix} 1 \ , \text{ se } k=l \\ 0 \ , \text{ se } k\neq l \end{matrix}\right. \ \ , \]
   em que $\delta_{k,l}$ é conhecido como \emph{delta de Kronecker}, e vale 1 se e somente se os seus dois índices são iguais; se forem diferentes, vale 0. 
   
   Um vetor $\ket{\psi}$ pode ser escrito como combinação linear dos vetores da base $\beta$ por $\ket{\psi} = \sum_k a_k \ket{b_k}$. Os coeficientes $a_k$ podem ser encontrados de maneira simples quando a base é ortonormal. Aplicando-se o produto interno em ambos os lados, tem-se
   \[ \braket{b_l}{\psi} = \bra{b_l} \left( \sum_k a_k \ket{b_k} \right) = \sum_k a_k \braket{b_l}{b_k} = \sum_k a_k \delta_{l,k} = a_l \ . \]
   
   Percebe-se que isso é análogo à decomposição de um vetor $\vectorarrow{v} \in \mathbb{R}^3$ nas suas componentes $x$, $y$ e $z$ na base canônica. Nesse caso, as componentes do vetor são dadas por $v_x = \vectorunit{x} \cdot \vectorarrow{v}$, $v_y = \vectorunit{y} \cdot \vectorarrow{v}$ e $v_z = \vectorunit{z} \cdot \vectorarrow{v}$, em que $\cdot$ denota o produto interno no $\mathbb{R}^3$. 
   
   Portanto, os coeficientes do vetor $\ket{\psi}$ na base ortonormal são obtidos realizando-se projeções de  $\ket{\psi}$ na direção dos vetores unitários $\ket{b_l}$ da base ortonormal. Assim:
     \[ \ket{\psi} = \sum_k \braket{b_k}{\psi} \ket{b_k} \ \  \Leftrightarrow \ \ \big[ \ket{\psi} \big]_\beta = \mqty[ \braket{b_0}{\psi}\\ \vdots \\ \braket{b_{n-1}}{\psi}]_\beta \ .    \]
   
   \begin{example}
    A base $\ket{0}$, $\ket{1}$ é ortonormal, em consequência dos exemplos \ref{cap2:ex_base_comp_norma_1} e \ref{cap2:ex_ortogonalidade_base_comp}. As projeções de um vetor $\ket{\psi} = a_0 \ket{0} + a_1 \ket{1}$ na base computacional são dadas por
    \begin{eqnarray*}
     a_0 &=& \braket{0}{\psi} \\
     a_1 &=& \braket{1}{\psi}
    \end{eqnarray*}
    e o vetor $\ket{\psi}$ pode ser escrito como
    \[ \ket{\psi} = \braket{0}{\psi} \ket{0} + \braket{1}{\psi} \ket{1} \ . \]
    \end{example}

    \begin{example}
    A base $\ket{+}$, $\ket{-}$ é ortonormal em consequência dos exemplos \ref{cap2:ex_base_comp_norma_1} e \ref{cap2:ex_ortogonalidade_base_x}. Os coeficientes do vetor $\ket{\psi} = \frac{1}{2} \left( \ket{0} + i \sqrt{3} \ket{1} \right)$ na base $\mathcal{X}$ são dados por:
    \begin{eqnarray*}
      a_0 
      &=& \braket{+}{\psi} \\
      &=& \frac{1}{\sqrt{2}} \ \frac{1}{2} \left( \bra{0} + \bra{1} \right) \left(  \ket{0} + i \sqrt{3} \ket{1} \right) \\
      &=& \frac{1}{2\sqrt{2}} (1 + i\sqrt{3} )
    \end{eqnarray*}
    \begin{eqnarray*}
      a_1
      &=& \braket{-}{\psi} \\
      &=& \frac{1}{\sqrt{2}} \ \frac{1}{2} \left( \bra{0} - \bra{1} \right) \left(  \ket{0} + i \sqrt{3} \ket{1} \right) \\
      &=& \frac{1}{2\sqrt{2}} (1 - i\sqrt{3} )
    \end{eqnarray*}
    Portanto, 
    \[ \ket{\psi} = \braket{+}{\psi} \ket{+} + \braket{-}{\psi} \ket{-} = \frac{1 + i\sqrt{3}}{2\sqrt{2}} \ket{+} + \frac{1 - i\sqrt{3}}{2\sqrt{2}} \ket{-} \ . \]
    \end{example}

    
  \end{subsection}

  \begin{subsection}{\nohyphens{Desigualdade de Cauchy-Schwarz}}
   Há uma desigualdade envolvendo normas de vetores que é válida de maneira geral, para quaisquer espaços vetoriais equipados com produto interno e norma. É conhecida por \emph{desigualdade de Cauchy-Schwarz}. 
   
   \begin{theorem}[Desigualdade de Cauchy-Schwarz]
    Dados $\ket{u}, \ket{v} \in V$, com $V$ um espaço vetorial qualquer munido de produto interno e norma, vale que
    \[ \abs{\braket{u}{v}} \leq \norm{\ket{u}} \norm{\ket{v}} \ . \]
    A igualdade ocorre se e somente se os vetores $\ket{u}$ e $\ket{v}$ forem múltiplos um do outro.
   \end{theorem}

   \begin{proof}
    Considere o vetor $a\ket{u} - b\ket{v}$. O produto interno desse vetor consigo mesmo deve ser real e não-negativo, por propriedade do produto interno, portanto 
    \[ \begin{split}
         0 
         &\leq \big( a^{*}\bra{u} - b^{*}\bra{v} \big) \big(a \ket{u} - b \ket{v} \big) \\
         &= \abs{a}^2\braket{u}{u} - a^{*}b\braket{u}{v} - a b^{*} \braket{v}{u} + \abs{b}^2\braket{v}{v} \ . \\
       \end{split} \]
    Escolhendo $a = \braket{v}{v}$ e $b = \braket{v}{u}$, tem-se
    \[ \begin{split}
        0 
        &\leq  \abs{a}^2\braket{u}{u} - a^{*}b\braket{u}{v} - a b^{*} \braket{v}{u}  + \abs{b}^2\braket{v}{v} \\
        &=   \abs{\braket{v}{v}}^2\braket{u}{u} - \braket{v}{v} \braket{v}{u} \braket{u}{v} - \braket{v}{v} \braket{u}{v} \braket{v}{u}  + \abs{\braket{v}{u}}^2\braket{v}{v} \\
        &=   \braket{v}{v}^2\braket{u}{u} - 2 \braket{v}{v} \braket{u}{v} \braket{v}{u}  + \abs{\braket{u}{v}}^2\braket{v}{v} \\
        &=   \braket{v}{v}^2\braket{u}{u} - 2 \braket{v}{v} \abs{ \braket{u}{v}}^2   + \abs{\braket{u}{v}}^2 \braket{v}{v} 
       \end{split} \]
   Logo, obtém-se
   \[ \begin{split}
        &0 
        \leq
        \braket{u}{u} \braket{v}{v} -  \abs{ \braket{u}{v}}^2 \\
       &\implies  \abs{ \braket{u}{v}} \leq \sqrt{\braket{u}{u} \braket{v}{v}} \\
       & \implies \abs{ \braket{u}{v}} \leq \norm{\ket{u}} \norm{\ket{v}} \ ,
       \end{split} \]
    e fica provada a desigualdade. 
    
    Ocorre igualdade se e somente se ocorrer igualdade em $ 0 \leq \big( a^{*}\bra{u} - b^{*}\bra{v} \big) \big(a \ket{u} - b \ket{v} \big)$, isto é, se e somente se o vetor $a \ket{u} - b \ket{v}$ for nulo\footnote{Isso é devido à propriedade (PI4) do produto interno.}. Portanto deve-se ter um dos vetores $\ket{u}$, $\ket{v}$ múltiplo do outro.
   \end{proof}

   No espaço vetorial $\mathbb{R}^n$ essa desigualdade permite definir o conceito de ângulo entre dois vetores por meio da relação $\cos \theta = \frac{\overrightarrow{u} \cdot \overrightarrow{v}}{\norm{\overrightarrow{u}} \norm{\overrightarrow{v}}}$. No caso de interesse para o presente trabalho, o do espaço vetorial $\mathbb{C}^n$, essa interpretação não é possível pois o produto interno pode assumir um valor complexo. 
   
  \end{subsection}

  \begin{subsection}{\nohyphens{Matriz de Mudança de Base entre Bases \\Ortonormais}}
   Da mesma forma que em \ref{cap2:matriz_de_mudança_de_base}, é possível escrever uma matriz que faz a mudança de base entre duas bases ortonormais $\beta_{\text{old}} = \big\{\ket{u_0}, \ldots, \ket{u_{n-1}} \big\} $ e $\beta_{\text{new}} = \big\{ \ket{v_0}, \ldots, \ket{v_{n-1}} \big\} $. As colunas da matriz de mudança de base são os vetores da base nova escritos como combinação linear dos vetores da base antiga. Como as bases são ortonormais, esses coeficientes podem ser obtidos pela projeção na direção dos vetores da base, como descrito na seção \ref{cap2:base_ortonormal}. 
   
   A matriz de mudança de base, nesse caso, é dada por
  \begin{eqnarray*}
  [I]^{\beta_{\text{old}}}_{\beta_{\text{new}} } 
  &=& 
         \mqty[ \Big{|} & \Big{|} & & \Big{|} 
         \\ \big[\ket{v_0}\big]_{\beta_{\text{old}}} &  \big[\ket{v_1}\big]_{\beta_{\text{old}}} & \cdots & \big[\ket{v_{n-1}}\big]_{\beta_{\text{old}}} \\ 
          \Big{|} & \Big{|} & & \Big{|} ] \\ \\
  &=& 
  \mqty[ \braket{u_0}{v_0} & \braket{u_0}{v_1}& \ \cdots \  & \braket{u_0}{v_{n-1}} \\
         \braket{u_1}{v_0} & \braket{u_1}{v_1}& \cdots & \braket{u_1}{v_{n-1}} \\
         \vdots & \vdots & \ddots  & \vdots \\
         \braket{u_{n-1}}{v_0} & \braket{u_{n-1}}{v_1}& \cdots & \braket{u_{n-1}}{v_{n-1}}  ]
  \ . 
  \end{eqnarray*}
  A matriz de mudança de base satisfaz $[I]^{\beta_{\text{old}}}_{\beta_{\text{new}} }\phantom{\big{|}}^{-1} = [I]^{\beta_{\text{old}}}_{\beta_{\text{new}} }\phantom{\big{|}}^{\dag} = [I]^{\beta_{\text{new}}}_{\beta_{\text{old}} } $. 
  Isso significa que a matriz de mudança de base é \emph{unitária}, um assunto que será visto na seção \ref{cap2:operadores_unitarios}.
   
   \begin{example}
    Sabendo que as bases $\mathcal{I}$ e $\mathcal{X}$ são ortonormais, pode-se encontrar a matriz de mudança de base fazendo as seguintes contas.
    \begin{eqnarray*}
     \braket{0}{+} &=& \bra{0} \left( \frac{1}{\sqrt{2}}\ket{0} +  \frac{1}{\sqrt{2}}\ket{0} \right) =  \frac{1}{\sqrt{2}} \\
     \braket{1}{+} &=& \bra{1} \left( \frac{1}{\sqrt{2}}\ket{0} +  \frac{1}{\sqrt{2}}\ket{0} \right) =  \frac{1}{\sqrt{2}} \\ \\
     \braket{0}{-} &=& \bra{0} \left( \frac{1}{\sqrt{2}}\ket{0} -  \frac{1}{\sqrt{2}}\ket{0} \right) =  \frac{1}{\sqrt{2}} \\
     \braket{1}{-} &=& \bra{1} \left( \frac{1}{\sqrt{2}}\ket{0} -  \frac{1}{\sqrt{2}}\ket{0} \right) = - \frac{1}{\sqrt{2}} \\
    \end{eqnarray*}
    Portanto, a matriz de mudança de base de $\mathcal{I}$ para $\mathcal{X}$ é dada por:
    \[ [I]^\mathcal{I}_\mathcal{X} = \mqty[ \braket{0}{+} & \braket{0}{-} \\ \braket{1}{+} & \braket{1}{-} ] = \frac{1}{\sqrt{2}} \mqty[ 1 & \phantom{-}1 \\ 1 & -1 ] = H \ .
    \]
    Esse é o mesmo resultado obtido no exemplo \ref{cap2:ex_matrix_mudança_de_base}.
   \end{example}

   
  \end{subsection}

  \begin{subsection}{Produto Exterior}
   É possível dar um significado ao símbolo $ \bra{\phi}$ como sendo um vetor linha. Isso permite, pois, considerar-se o produto matricial $\ket{\psi} \cdot \bra{\phi} = \op{\psi}{\phi}$ de um vetor linha $n\times 1$ por um vetor coluna $1 \times n$, resultando em uma matriz $n \times n$. Essa operação é chamada de \emph{produto exterior}.
   
   \begin{example}
    No espaço vetorial de 1 qubit, alguns exemplos de produto exterior são:
    \begin{eqnarray*}
      \op{0}{0} &=& \mqty[ 1 \\ 0 ] \cdot \mqty[ 1 & 0 ] = \mqty[ 1 & 0 \\ 0 & 0 ]  \\
      \op{0}{1} &=& \mqty[ 1 \\ 0 ] \cdot \mqty[ 0 & 1 ] = \mqty[ 0 & 1 \\ 0 & 0 ]  \\
      \op{1}{0} &=& \mqty[ 0 \\ 1 ] \cdot \mqty[ 1 & 0 ] = \mqty[ 0 & 0 \\ 1 & 0 ]  \\
      \op{1}{1} &=& \mqty[ 0 \\ 1 ] \cdot \mqty[ 0 & 1 ] = \mqty[ 0 & 0 \\ 0 & 1 ]  
    \end{eqnarray*}
   
     \begin{eqnarray*}
      \op{+}{0} &=& \mqty[ \tfrac{1}{\sqrt{2}} \\ \tfrac{1}{\sqrt{2}} ] \cdot \mqty[ 1 & 0 ] = \mqty[ \tfrac{1}{\sqrt{2}} & 0 \\ \tfrac{1}{\sqrt{2}} & 0 ]  \\
      \op{+}{-} &=& \mqty[ \tfrac{1}{\sqrt{2}} \\ \tfrac{1}{\sqrt{2}} ] \cdot \mqty[ \tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}} ] = \mqty[ \tfrac{1}{2} & -\tfrac{1}{2} \\ \tfrac{1}{2}  & -\tfrac{1}{2}  ] 
     \end{eqnarray*}

   \end{example}

  \end{subsection}

  
  \end{section}

 
  \begin{section}{Transformações Lineares}
  
  \begin{subsection}{\nohyphens{Transformação Linear e Operador Linear}}
   Uma \emph{transformação linear} é uma aplicação $T \colon \mathbb{C}^n \to \mathbb{C}^m$ que respeita a soma e a multiplicação por escalar, ou seja, tal que valem:
   \begin{itemize}
    \item[(TL1)] Preservação da soma: \[ T(\ket{\phi} + \ket{\psi}) = T\ket{\phi} + T\ket{\psi} \]
    \item[(TL2)] Preservação do produto por escalar: \[ T( z\ket{\psi} ) = z \cdot T\ket{\psi} \ . \]
   \end{itemize}
    Um \emph{operador linear} é uma transformação linear $A \colon \mathbb{C}^n \to \mathbb{C}^n$ ($m=n$). 
    
    \begin{example}\label{cap2:ex_verificar_transformacao_linear}
     A função $H \colon \mathbb{C}^2 \to \mathbb{C}^2$ dada por 
     \[ H \left( a_0 \ket{0} + a_1 \ket{1} \right) = \frac{a_0 + a_1}{\sqrt{2}} \ket{0} + \frac{a_0-a_1}{\sqrt{2}} \]
     é uma transformação linear. De fato, as propriedades de transformação linear se verificam para $H$.
     
     \noindent \textbf{Preservação da soma:} Sejam $\ket{\phi} = a_0 \ket{0} + a_1 \ket{1}$ e $\ket{\psi} = b_0 \ket{0} + b_1 \ket{1}$. 
     \begin{eqnarray*} 
      H(\ket{\phi} + \ket{\psi}) 
      &=& H \left( a_0 \ket{0} + a_1 \ket{1} + b_0 \ket{0} + b_1 \ket{1} \right)  \\
      &=& H\left( (a_0 + b_0) \ket{0} + (a_1 + b_1) \ket{1} \right) \\
      &=& \frac{(a_0 + b_0) + (a_1 + b_1)}{\sqrt{2}} \ket{0} + \frac{(a_0 + b_0) - (a_1 + b_1)}{\sqrt{2}}   \\ 
      &=& \frac{a_0 + a_1}{\sqrt{2}} \ket{0} + \frac{a_0-a_1}{\sqrt{2}} + \frac{b_0 + b_1}{\sqrt{2}} \ket{0} + \frac{b_0-b_1}{\sqrt{2}}  \\
      &=& H \ket{\phi} + H \ket{\psi} 
     \end{eqnarray*}
    
     \noindent \textbf{Preservação do produto por escalar:} Sejam $z \in \mathbb{C}$ e $\ket{\psi} = a_0 \ket{0} + a_1 \ket{1}$. 
     \begin{eqnarray*} 
      H(z \ket{\psi}) 
      &=& H \big( z(a_0 \ket{0} + a_1 \ket{1}) \big) \\
      &=&  H \left( za_0 \ket{0} + za_1 \ket{1}) \right) \\
      &=& \frac{za_0 + za_1}{\sqrt{2}} \ket{0} + \frac{za_0-za_1}{\sqrt{2}} \\
      &=& z \left( \frac{a_0 + a_1}{\sqrt{2}} \ket{0} + \frac{a_0-a_1}{\sqrt{2}} \right) \\
     &=& z\cdot  H \ket{\psi} 
     \end{eqnarray*}
    
    \end{example}


   \end{subsection}
   
   \begin{subsection}{Funcional Linear}
   Um \emph{funcional linear} é uma transformação linear $f \colon \mathbb{C}^n \to \mathbb{C}$ ($m=1$). O bra $\bra{\phi}$ pode ser pensado como um funcional linear que pode atuar em um vetor coluna $\ket{\psi}$ para resultar no número complexo $\braket{\phi}{\psi}$. Pode-se verificar que todo funcional linear é da forma $\bra{\phi} = \braket{\phi}{\cdot}$ para algum $\ket{\phi}$. 
   
   \begin{example}
    O bra $\bra{0}$ é um funcional linear que leva $\ket{\psi}$ no coeficiente $\braket{0}{\psi}$ da projeção na direção $\ket{0}$. Igualmente, o bra $\bra{1}$ é um funcional linear que leva $\ket{\psi}$ no coeficiente $\braket{1}{\psi}$ da projeção na direção $\ket{1}$.
   \end{example}

   \end{subsection}
   
   \begin{subsection}{\nohyphens{Projeção e Relação de Completude}}
   Se $\ket{u}$ for unitário, o funcional linear $\bra{u}$ leva um ket $\ket{\psi}$ em $\braket{u}{\psi}$, que corresponde ao coeficiente da projeção de $\ket{\psi}$ na direção de $\ket{u}$. 
   
   O vetor $\braket{u}{\psi}\ket{u}$ é a projeção de $\ket{\psi}$ na direção do vetor unitário $\ket{u}$. Movendo-se o número $\braket{u}{\psi}$ para a direita, pode-se escrever essa projeção como $\text{proj}_{\ket{u}} \ket{\psi} = \op{u}{u}\ket{\psi}$. O operador $\op{u}{u}$ é, então, chamado operador projeção na direção de $\ket{u}$
   
   Se $\beta = \{\ket{b_0}, \ldots, \ket{b_{n-1}} \} $ é uma base ortonormal, pode-se escrever qualquer vetor $\ket{\psi}$ como soma das suas projeções ortogonais sobre as direções definidas pelos vetores da base. Dessa forma, tem-se
   \[ \ket{\psi} = \sum_{k=0}^{n-1} \braket{b_k}{\psi} \ket{b_k} =  \sum_{k=0}^{n-1} \op{b_k}{b_k} \ket{\psi}  \ \ .  \]
   Segue que 
   \[ \sum_{k=0}^{n-1} \op{b_k}{b_k} = I  \ ,  \]
   expressão conhecida como \emph{relação de completude}.

   \begin{example}
    A projeção ortogonal da direção do vetor $\ket{0}$ é o operador $\op{0}{0}$, visto que sua ação em um ket $\ket{\psi}$ é dada por $\op{0}{0} \ket{\psi} = \braket{0}{\psi} \ket{0}$ e a projeção ortogonal na direção de $\ket{1}$ é o operador de projeção $\op{1}{1}$, pois $\op{1}{1} \ket{\psi} = \braket{1}{\psi} \ket{1}$.
    
    A relação de completude no espaço vetorial dos estados de 1 qubit, $\mathbb{C}^2$, é dada por 
    \[ I = \op{0}{0} + \op{1}{1} \ . \]
    
   \end{example}

   \end{subsection}
   
   \begin{subsection}{\nohyphens{Definição de uma Transformação Linear nos \\Elementos da Base}}\label{cap2:sec_def_tl_nos_elementos_da_base}
   Para definir uma transformação linear, basta que se forneça como ela atua nos elementos de uma base. Isto é, dada $\beta = \{\ket{b_0}, \ldots, \ket{b_{n-1}} \} $ base de $\mathbb{C}^n$, pode-se obter $T\ket{\psi}$ conhecendo-se $T\ket{b_k}$ para todo $k$. De fato, como $\beta$ base, pode-se escrever $\ket{\psi}$ como combinação linear 
   \[ \ket{\psi} = \sum_k a_k \ket{b_k} \ \ .\]
   Aplicando-se a transformação $T$ e usando a linearidade, obtém-se
   \[ T\ket{\psi} = \sum_k a_k T\ket{b_k}  \ \ ,\]
   e dessa forma, $T\ket{\psi}$ pode ser obtido a partir dos $T\ket{b_k}$'s.
   
   \begin{example}\label{cap2:ex_transformacao_linear_nos_elementos_da_base}
    Considere o operador linear em 1 qubit $X \colon \mathbb{C}^2 \to \mathbb{C}^2$ definido nos vetores da base computacional por 
    \[ \begin{array}{l}
         X\ket{0} = \ket{1} \\
         X \ket{1} = \ket{0}
       \end{array} \]
    O operador $X$ está bem definido em todo $\ket{\psi} = a \ket{0} + b \ket{1}$ graças à sua linearidade:
    \[ X\ket{\psi} = X\big(a \ket{0} + b \ket{1}\big) = a X\ket{0} + b X \ket{1} = a \ket{1} + b \ket{0} \ . \]
   \end{example}

  \end{subsection}
  
   \begin{subsection}{\nohyphens{Matriz de uma Transformação Linear}}
   Seja $T  \colon U= \mathbb{C}^n \to V=\mathbb{C}^m$  uma transformação linear. Sejam $\beta_U = \{\ket{u_0}, \ldots, \ket{u_{n-1}} \} $ base de $U$ e $\beta_V = \{ \ket{v_0} , \ldots , \ket{v_{n-1}} \} $ base de $V$. Quando fixadas bases para os espaços vetoriais do domínio e do contradomínio de $T$, é possível representar a transformação $T$ por uma matriz, de forma que a atuação de $T$ sobre um vetor $\ket{\psi}$ é equivalente ao produto matriz-vetor coluna.    
   
   A matriz da transformação linear $T$ nas bases $\beta_U$ e $\beta_V$ é dada por: 
\[ [T]^{\beta_U}_{\beta_V} = \left[ \begin{matrix} | & & | \\ [T(u_0)]_{\beta_V} & \cdots & [T(u_{n-1})]_{\beta_V} \\  | & & | \end{matrix} \right]  \in M(m,n)  \]
   Definida dessa forma, vale que:
   \[ [T\ket{\psi}]_{\beta_V} = [T]^{\beta_U}_{\beta_V} \cdot [\ket{\psi}]_{\beta_U} \ \ ,\]
   portanto, a atuação da matriz de $T$ sobre um ket é equivalente à multiplicação matriz-vetor coluna levando-se em consideração as bases previamente fixadas. 
   
   Considere que as bases $\beta_U$ e $\beta_V$ sejam ortonormais. Cada vetor $T\ket{u_k}$ pode ser escrito na base $\beta_V$ da seguinte forma:
   \[ [T\ket{u_k}]_{\beta_V} = \mqty[ \braket{v_0}{Tu_k} \\  \braket{v_1}{Tu_k} \\ \vdots \\  \braket{v_{m-1}}{Tu_k} ]_{\beta_V} \ , \]
   tendo em vista que a $l$-ésima entrada do vetor é o coeficiente da projeção de $\ket{Tu_k}$ na direção do $l$-ésimo vetor da base em $V$. Assim, a entrada de linha $l$ e coluna $k$ da matriz $[T]^{\beta_U}_{\beta_V}$ é $\braket{v_l}{Tu_k}$, com $l = 0, \ldots , m-1$ e $k=0, \ldots , n-1$ e consequentemente
   \[ [T]^{\beta_U}_{\beta_V} = 
   \mqty[ \braket{v_0}{Tu_0} & \braket{v_0}{Tu_1} & \ldots & \braket{v_0}{Tu_{n-1}} \\
          \braket{v_1}{Tu_0} & \braket{v_1}{Tu_1} & \ldots & \braket{v_1}{Tu_{n-1}} \\
          \vdots & \vdots & \ddots & \vdots \\
          \braket{v_{m-1}}{Tu_0} & \braket{v_{m-1}}{Tu_1} & \ldots & \braket{v_{m-1}}{Tu_{n-1}} ] \ .
          \]
          
   No caso de um operador linear, tem-se $U=V$ ($m=n$), e é possível escolher a mesma base $\beta = \{\ket{b_0}, \ldots, \ket{b_{n-1}} \} $ para o domínio e o contradomínio da transformação. Essa é uma situação bastante frequente, e a matriz associada ao operador linear é montada da seguinte forma: as colunas da matriz são os vetores $\ket{Tb_k}$ escritos como vetores coluna na base $\beta$. Portanto:
\[ [T]_{\beta} = [T]^{\beta}_{\beta} = \left[ \begin{matrix} | & & | \\ \big[\ket{Tb_0}\big]_{\beta} & \cdots & \big[\ket{Tb_{n-1}}\big]_{\beta} \\  | & & | \end{matrix} \right]  \]

  Se a base $\beta$ for ortonormal, obtém-se que
  \[ [T]_{\beta} = 
   \mqty[ \braket{b_0}{Tb_0} & \braket{b_0}{Tb_1} & \ldots & \braket{b_0}{Tb_{n-1}} \\
          \braket{b_1}{Tb_0} & \braket{b_1}{Tb_1} & \ldots & \braket{b_1}{Tb_{n-1}} \\
          \vdots & \vdots & \ddots & \vdots \\
          \braket{b_{n-1}}{Tb_0} & \braket{b_{n-1}}{Tb_1} & \ldots & \braket{b_{n-1}}{Tb_{n-1}} ] \ .
          \]

   
   \begin{example}
    Um operador linear $A$ sobre um qubit pode ser escrito como uma matriz (na base computacional) $2\times2$ com coeficientes complexos da seguinte forma:
   \[ [A] = \left[ \begin{matrix} | & | \\ \big[ A\ket{0}\big]  & \big[ A\ket{1}\big] \\  | & | \end{matrix} \right] = 
    \mqty[ \phantom{\Big(} \! \! \! \mel{0}{A}{0} & \mel{0}{A}{1} \ \ \\
            \phantom{\Big(}\! \! \! \mel{1}{A}{0} & \mel{1}{A}{1} \ \  ]
          \ \ , \]
   com $ [\cdot] $ significando que os vetores em questão estão escritos como vetores coluna na base computacional. É frequente denotar a matriz do operador $A$ pelo mesmo símbolo $A$, quando está implícito qual base é considerada.
   \end{example}

   \begin{example}\label{cap2:ex_matriz_da_transformacao_linear}
    A matriz da transformação linear do exemplo \ref{cap2:ex_transformacao_linear_nos_elementos_da_base}, na base computacional, é obtida escrevendo-se a ação de $X$ sobre os vetores da base.
     \[ \begin{array}{l}
         X\ket{0} = \ket{1} = \mqty[ 0 \\ 1 ] \\ \\
         X \ket{1} = \ket{0} = \mqty[ 1 \\ 0 ] \ .
       \end{array} \]
    Em seguida, monta-se a matriz fazendo
    \[ X = \left[ \begin{matrix} | & | \\  X\ket{0}  &  X\ket{1} \\  | & | \end{matrix} \right] = \mqty[ 0 & 1 \\ 1 & 0 ] \ . \]
   \end{example}

     \begin{example}[Matrizes de Pauli]\label{cap2:ex_matrizes_de_pauli}
     As matrizes
     \[ X = \mqty[0 & 1 \\ 1 & 0 ] \ \ , \ \ \ Y = \mqty[ 0 & -i \\ i & \phantom{-}0 ] \ \ \text{e} \ \ \ Z =  \mqty[ 1 & \phantom{-}0 \\ 0 & -1 ] \]
     são conhecidas como \emph{matrizes de Pauli}. Essas são representações na base computacional dos operadores $X$, $Y$ e $Z$. Usa-se, costumeiramente, a mesma notação para se referir ao operador e à sua matriz na base computacional. 
     
      Em determinadas situações, a matriz identidade $I$ também é chamada matriz de Pauli, e usa-se a notação alternativa 
     \[ I = \sigma_0 \ \ , \ \ \ X = \sigma_x = \sigma_1 \ \ , \ \ \ Y = \sigma_y = \sigma_2 \ \ \text{e} \ \ \ Z = \sigma_z = \sigma_3 \ . \]
    \end{example}
    
  \end{subsection}

  \begin{subsection}{\nohyphens{Matriz da Composição de Transformações \\Lineares}}
   A composição de transformações lineares $T \colon U \to V$ e $R \colon V \to W$ é a transformação linear denotada por $R T = R \circ T \colon U \to W$ e tal que $RT(\ket{\psi}) = R \Big( T\big(\ket{\psi}\big) \Big)$ para todo $\ket{\psi}$. A matriz dessa transformação linear pode ser obtida pela multiplicação matricial das matrizes de $R$ e de $T$:
   \[ [RT]^{\beta_U}_{\beta_W} = [R]^{\beta_U}_{\beta_V} \cdot [T]^{\beta_V}_{\beta_W} \ , \]
   em que $\beta_U$, $\beta_V$ e $\beta_W$ são bases de $U$, $V$ e $W$, respectivamente. 
  \end{subsection}

   \begin{subsection}{Mudança de Base}
     Para escrever a matriz de uma transformação linear $T\colon U \to V$ em novas bases $\beta_U^\prime$ e $\beta_V^\prime$ basta aplicar matrizes de mudança de base de maneira apropriada. 
     \[ [T]^{\beta_U^\prime}_{\beta_V^\prime} = [I]^{\beta_U^\prime}_{\beta_U} [T]^{\beta_U}_{\beta_V} [I]^{\beta_V}_{\beta_V^\prime} \ .  \] 
     
     No caso de um operador linear $A \colon V \to V$, pode-se usar a mesma base nos espaços vetoriais do domínio e do contradomínio da função. A mudança de base nesse caso fica:
     \[ [A]^{\beta_U^\prime}_{\beta_U^\prime} = [I]^{\beta_U^\prime}_{\beta_U} [A]^{\beta_U}_{\beta_U} [I]^{\beta_U}_{\beta_U^\prime}
      = \big( [I]^{\beta_U}_{\beta_U^\prime} \big)^{-1} [A]^{\beta_U}_{\beta_U} \big( [I]^{\beta_U}_{\beta_U^\prime} \big) \ .
     \]
    
    A transformação matricial $[A] \to [A]^\prime = [M]^{-1} [A] [M]$ é conhecida como \emph{transformação de similaridade}. Duas transformações conectadas dessa forma são ditas \emph{matrizes semelhantes}. As matrizes semelhantes são representantes de um mesmo operador linear escrito em bases diferentes. 
    
    Se as bases $\beta_U$ e $\beta_U^\prime$ forem ortonormais, a fórmula para mudança de base fica:
    \[ [A]^{\beta_U^\prime}_{\beta_U^\prime} = [I]^{\beta_U^\prime}_{\beta_U} [A]^{\beta_U}_{\beta_U} [I]^{\beta_U}_{\beta_U^\prime}
      = \big( [I]^{\beta_U}_{\beta_U^\prime} \big)^{\dag} [A]^{\beta_U}_{\beta_U} \big( [I]^{\beta_U}_{\beta_U^\prime} \big) \ ,
     \]
     em que a operação simbolizada por $\dag$ é a transposição e conjugação da matriz. Essa operação será introduzida formalmente na seção \ref{cap2:operador_adjunto}.
     
     \begin{example}\label{cap2:ex_matriz_hadamard_mudança_base_x}
      Considere as bases $\mathcal{I}$ e $\mathcal{X}$ apresentadas no exemplo \ref{cap2:bases_1qubit}. 
      A matriz de mudança de base de $\mathcal{I}$ para $\mathcal{X}$ e vice-versa é a matriz de Hadamard $H$, como visto no exemplo \ref{cap2:ex_matrix_mudança_de_base}. O operador $X$, visto no exemplo \ref{cap2:ex_matrizes_de_pauli}, cuja matriz na base computacional é
      \[ X = [X]_{\mathcal{I}} =  \mqty[ 0 & 1 \\ 1 & 0 ] \]
      pode ser representado na base $\mathcal{X}$ por
      \[ \begin{split}
    [X]_{\mathcal{X}} 
    &= [I]^{\mathcal{I}}_{\mathcal{X}} [X]_\mathcal{I} [I]^{\mathcal{X}}_{\mathcal{I}} \\
    &= H X H \\
    &= \frac{1}{\sqrt{2}}\mqty[ 1 & 1 \\ 1 & -1 ] \mqty[ 0 & 1 \\ 1 & 0 ] \frac{1}{\sqrt{2}}\mqty[ 1 & 1 \\ 1 & -1 ]  \\
    &= \mqty[ 1 & 0 \\ 0 & -1 ] \\
    &= Z \ .
         \end{split} \]
     \end{example}

   \end{subsection}
    
  \end{section}


 \begin{section}{\nohyphens{Autovalores, Autovetores e Decomposição Espectral}}
 
 \begin{subsection}{Autovalores e Autovetores}
  Seja $A$ um operador linear, com matriz na base computacional também representada por $A$. Os \emph{autovalores} de $A$ são os números complexos $\lambda$ que satisfazem
  \[ A \ket{v} = \lambda \ket{v} \ \  \text{para algum $\ket{v} \neq 0$ .} \]
  Os vetores não nulos $\ket{v}$ que satisfazem a equação acima são chamados \emph{autovetores} de $A$ associados ao autovalor $\lambda$.
  
 \begin{example}\label{cap2:ex_autovalores_z}
  O operador linear em 1 qubit $Z$ definido pela matriz
  \[ Z = \mqty[ 1 & 0 \\ 0 & -1 ] \]
  possui:
  \begin{itemize}
   \item autovalor $1$, pois o vetor não nulo $\ket{0}$ é tal que $Z \ket{0} = 1 \cdot \ket{0}$;
   \item autovalor $-1$, pois o vetor não nulo $\ket{1}$ satisfaz $Z \ket{1} = -1 \cdot \ket{1}$.
  \end{itemize}

 \end{example}

 \end{subsection}
 
 \begin{subsection}{Cálculo de Autovalores}
  A equação de autovalores $A \ket{v} = \lambda \ket{v}$ é equivalente a $\big(A-\lambda I \big) \ket{v} = 0$, com $\ket{v} \neq 0 $, e isso é equivalente a dizer que a matriz de $A-\lambda I $ é singular. Por sua vez, isso equivale à equação
   \[  \det (A-\lambda I ) = 0 \  . \]
  Com a equação acima, consegue-se encontrar os autovalores $\lambda$ do operador $A$ encontrando-se as raízes do \emph{polinômio característico} $(A-\lambda I)$. Esse polinômio têm grau $n$ e, como estamos buscando raízes nos números complexos, admite $n$ raízes (pode acontecer que sejam repetidas). Dessa forma, todo operador admite um autovalor\footnote{Isso não é necessariamente válido para espaços vetoriais reais.}.
  
  \begin{example}
   Os autovalores da matriz 
   \[ A = \mqty[ 0 & 2 \\ -1 & i ] \]
   podem ser obtidos por:
   \begin{eqnarray*}
  & & \det (A-\lambda I ) = 0 \\
  & & \det \mqty[ 0 - \lambda & 2 \\ -1 & i-\lambda ] = 0 \\
  & & -\lambda(i-\lambda) - 2 \cdot -1 = 0\\
  & & \lambda^2 - i \lambda + 2 = 0 \\
  & & \lambda = \dfrac{i \pm \sqrt{i^2 - 4 \cdot 1 \cdot 2}}{2\cdot 1} \\
  & & \lambda = -i \ \text{ ou } \  \lambda = 2i \ .
   \end{eqnarray*}
   Os autovalores podem ser números complexos. Como a matriz é $2\times2$, foi obtido um polinômio característico de grau 2 e foram obtidas 2 raízes. 
  \end{example}

  \end{subsection}
 
 \begin{subsection}{Cálculo de Autovetores}
 Uma vez descobertos os autovalores $\lambda$, retorna-se à equação $A \ket{v} = \lambda \ket{v}$, ou melhor, à equação
 \[ \big(A-\lambda I \big) \ket{v} = 0 \]
 para encontrar todos os autovetores $\ket{v} \neq 0$ satisfazendo essa equação. Como a matriz $A-\lambda I $ é singular (essa é a condição para se encontrar $\lambda$), a equação em questão admite infinitas soluções $\ket{v}$, formando um sistema linear possível e indeterminado. 
 
 Em algumas situações é possível montar uma base para o espaço composta por autovetores do operador $A$. Há condições sobre o operador que revelam se é possível obter uma base ortonormal de autovetores. Isso será visto na seção \ref{cap2:tipos_especiais_de_operadores}. A obtenção de uma base de autovetores permite escrever a matriz $A$ nessa base como uma matriz diagonal, o que se prova útil em diversas circunstâncias. 

 \begin{example}\label{cap2:ex_autovetores_X}
  Dada a matriz 
  \[ X = \mqty[ 0 & 1 \\ 1 & 0 ] \ , \]
  os autovalores e autovetores são encontrados a seguir.
  
  \noindent \textbf{Autovalores:} Resolvendo $\det(X - \lambda I) = 0$, obtém-se:
  \begin{eqnarray*}
  & & \det(X - \lambda I) = 0 \\
  & &  \det \mqty[ -\lambda & 1 \\ 1 & -\lambda ] = 0 \\
  & &  \lambda^2 - 1 = 0  \\
  & &  \lambda = \pm 1 \ .
  \end{eqnarray*}

  \noindent \textbf{Autovetores:} Para cada autovalor $\lambda$, deve-se resolver 
  \[ \big(X-\lambda I \big) \ket{v} = 0 \ , \]
  obtendo-se o vetor $\ket{v}$.
  \\ \\
  \noindent Para $\lambda = -1$:
  \begin{eqnarray*}
   \big(X-\lambda I \big) \ket{v} &=& 0 \\
   \mqty[ -1 & 1 \\ 1 & -1 ] \mqty[ a_0 \\ a_1] &=& \mqty[ 0 \\ 0 ] 
  \end{eqnarray*}
   \[ \begin{cases}
        -a_0 + a_1 = 0 \\ \phantom{-}a_0 - a_1 = 0
       \end{cases} \]
   
   O sistema resultante, como esperado, é possível e indeterminado. Resolvendo o sistema, tem-se:
   \[ \begin{cases}
       a_0 = a_1 \\
       a_1 \in \mathbb{C} \ .
      \end{cases} \]
   
   Os autovetores associados ao autovalor $\lambda = -1$ são:
    \[ \ket{v} = \mqty[ a_1 \\ a_1 ] = a_1 \mqty[ 1 \\ 1 ]  \ \ \text{ com } a_1 \in \mathbb{C}, a_1 \neq 0 \]
    
    O autoespaço associado a $\lambda = -1$ é o subespaço vetorial:
    \[ V_{-1} = \left\lbrace a_1 \mqty[ 1 \\ 1 ]  \colon a_1 \in \mathbb{C} \right\rbrace \ = \text{span}\left\lbrace \mqty[ 1 \\ 1 ] \right\rbrace . \]
    \\ \\    
    \noindent Para $\lambda = 1$: 
    \begin{eqnarray*}
   \big(X-\lambda I \big) \ket{v} &=& 0 \\
   \mqty[ 1 & 1 \\ 1 & 1 ] \mqty[ a_0 \\ a_1] &=& \mqty[ 0 \\ 0 ] 
  \end{eqnarray*}
   \[ \begin{cases}
        a_0 + a_1 = 0 \\ a_0 + a_1 = 0
       \end{cases} \]
   
   O sistema é possível e indeterminado, e, resolvendo o sistema, tem-se:
   \[ \begin{cases}
       a_0 \in \mathbb{C}  \\
       a_1 = - a_0 \ .
      \end{cases} \]
   
   Os autovetores associados ao autovalor $\lambda = 1$ são:
    \[ \ket{v} = \mqty[ a_0 \\ - a_0 ] = a_0 \mqty[\phantom{-} 1 \\ - 1 ]  \ \ \text{ com } a_0 \in \mathbb{C}, a_0 \neq 0 \]
    
    O autoespaço associado a $\lambda = 1$ é o subespaço vetorial:
    \[ V_{1} = \left\lbrace a_0 \mqty[ \phantom{-}1 \\ -1 ]  \colon a_0 \in \mathbb{C} \right\rbrace \ = \text{span}\left\lbrace \mqty[ \phantom{-}1 \\ -1 ] \right\rbrace . \]
 \end{example}

 \end{subsection}

 \begin{subsection}{Diagonalização de Operadores}
 Uma vez encontrados os autovalores $\lambda_j$ e uma base de autovetores $\mathcal{V} = \{ \ket{v_j} , j=0, \ldots , n-1 \}$, com $\ket{v_j}$ associado ao autovalor $\lambda_j$, o operador linear $A$ pode ser escrito na base $\mathcal{V}$ como uma matriz diagonal. Defina as matrizes:
 \begin{eqnarray*}
  D &=& \mqty[  \ \lambda_0 \  & & \\  & \ \ddots \ & \\ & & \ \lambda_{n-1}  \, \, ] \text{(matriz diagonal dos autovalores)} \\
  M &=& \mqty[ | & & | \\ \ket{v_0} & \cdots & \ket{v_{n-1}} \\  | & & |  ]  \text{(matriz de mudança de base: $\mathcal{I} \ \rightarrow  \ \mathcal{V}$)} \ .
\end{eqnarray*}
 
 A matriz de $A$ na base $\mathcal{V}$ é dada por:
  \begin{eqnarray*}
  [A]_{\mathcal{V}}
  &=& 
  \mqty[ | & & | \\ {[A\ket{v_0}]_{\mathcal{V}}} \ \  & \cdots & \ \ \ \  {[A\ket{v_{n-1}}]_{\mathcal{V}}}\,  \\ | &  & | ] \\
  &=& 
 \mqty[ | & & | \\ {[\lambda_0\ket{v_0}]_{\mathcal{V}}}  & \cdots & {[\lambda_{n-1}\ket{v_{n-1}}]_{\mathcal{V}}} \\ | &  & | ] \\
  &=& 
 \mqty[  \lambda_0  & & \\  & \ddots & \\ & & \lambda_{n-1} ] = D  
  \end{eqnarray*}
  
  A matriz $M$ nada mais é que a matriz de mudança de base $M=[I]^{\mathcal{I}}_{\mathcal{V}}$. Conforme a seção \ref{cap2:matriz_de_mudança_de_base} , pode-se escrever:
  \[ D = [A]_\mathcal{V} = [I]^{\mathcal{V}}_{\mathcal{I}} [A] [I]_{\mathcal{V}}^{\mathcal{I}} = M^{-1} A M \ .\]
  Ainda, se a base $\mathcal{V}$ for ortonormal, conforme \ref{cap2:base_ortonormal}, pode-se escrever
  \[ D = M^\dag A M \ . \]
  Nessas expressões, usa-se $A$ para denotar a matriz $[A]$ do operador $A$ na base computacional, o que simplifica a notação quando não houver risco de confusão.  
  
  Portanto, de posse dos autovalores e de uma base ortonormal, é possível escrever o operador $A$ como uma matriz diagonal. 
  
  O operador $A$ também pode ser representado na notação do produto exterior da seguinte forma. Como $\mathcal{V}$ forma uma base ortonormal, vale a relação de completude $I = \sum_k \op{v_k}{v_k} $ para essa base. Aplicando-se essa relação a $A$, obtém-se
  \[ A = AI = \sum_k A \op{v_k}{v_k} = \sum_k \lambda_k \op{v_k}{v_k} \ . \]


  \begin{example}\label{cap2:ex_diagonalizacao_X}
   No exemplo \ref{cap2:ex_autovetores_X} foram calculados os autovalores e autovetores da matriz
   \[ X = \mqty[ 0 & 1 \\ 1 & 0 ] \ , \]
   obtendo-se:
   \begin{itemize}
    \item $\lambda = 1\phantom{-}$ , \ \ $\ket{v} = a \mqty[ 1 \\ 1 ]$ \ \ $\phantom{-}$($a \in \mathbb{C}, a \neq 0$)
    \item $\lambda = -1$ , \ \ $\ket{v} = a \mqty[\phantom{-}1 \\ -1 ]$ \ \ ($a \in \mathbb{C}, a \neq 0$)
   \end{itemize}
   
   Pretende-se extrair uma base ortonormal de autovetores para escrever $X$ na forma diagonal. Nesse caso\footnote{Todos os autoespaços de dimensão 1.}, basta normalizar os autovetores obtidos.
   \\ \\
   \noindent Para $\lambda = -1$: 
   \[ \norm{a \mqty[ 1 \\ 1 ] } = 1 \implies \abs{a} \sqrt{1^2 + 1^2} = 1 \implies \abs{a} = \frac{1}{\sqrt{2}} \ . \]
   Há várias opções para $a$ que satisfazem essa condição. Pode-se escolher apenas uma delas para fazer a diagonalização, por exemplo: $a = \frac{1}{\sqrt{2}}$. O autovetor normalizado é, portanto,
   \[ \ket{v} = \frac{1}{\sqrt{2}} \mqty[ 1 \\ 1 ] = \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt{2}} \ket{1} \ . \]
   \\ \\
   \noindent Para $\lambda = 1$: 
   \[ \norm{a \mqty[ \phantom{-}1 \\ -1 ] } = 1 \implies \abs{a} \sqrt{1^2 + (-1)^2} = 1 \implies \abs{a} = \frac{1}{\sqrt{2}} \ . \]
   Do mesmo modo, pode-se escolher $a = \frac{1}{\sqrt{2}}$, e o autovetor normalizado é:
   \[ \ket{v} = \frac{1}{\sqrt{2}} \mqty[\phantom{-} 1 \\ -1 ] = \frac{1}{\sqrt{2}} \ket{0} - \frac{1}{\sqrt{2}} \ket{1} \ . \]
  \\ \\ 
  \noindent \textbf{Base ortonormal de autovetores:} 
  \[ \underbrace{\ket{v_0} = \frac{1}{\sqrt{2}} \mqty[ 1 \\ 1 ]}_{\lambda = 1} \ \ , \ \ \underbrace{\ket{v_1} = \frac{1}{\sqrt{2}} \mqty[\phantom{-} 1 \\ -1 ]}_{\lambda = -1} \]
  \\ \\ 
  \noindent \textbf{Diagonalização da matriz:}
  
  Matriz de $X$ na forma diagonal:
  \[ X_D = \mqty[\lambda_0 & \\ & \lambda_1] =  \mqty[1 & \\ & -1 ] \]
  Matriz de mudança de base:
  \[ M = \mqty[ | & | \\ \ket{v_0}  & \ket{v_{1}} \\ | & | ] = \mqty[ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ & \\ \phantom{-}\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} ] = \frac{1}{\sqrt{2}} \mqty[ 1 & \phantom{-}1 \\ 1 & -1 ] \]
    \\ \\ 
  \noindent \textbf{Operador na forma diagonal:} Na notação de produto exterior, tem-se:
  \[ X = \sum_k \lambda_k \op{v_k}{v_k} = 1 \cdot \op{v_0}{v_0} - 1 \cdot \op{v_1}{v_1} \]
  
  \end{example}

 
 \end{subsection}

 
 \end{section}
 
 
 \begin{section}{\nohyphens{Tipos Especiais de Operadores}}\label{cap2:tipos_especiais_de_operadores}
  
  Nesta seção, alguns operadores com propriedades especiais serão apresentados. Os operadores normais, hermitianos, unitários, positivos e projetivos participam da base da Mecânica Quântica. 
  
  \begin{subsection}{Operador Adjunto}\label{cap2:operador_adjunto}
   Seja $A$ um operador linear. Em Álgebra Linear, é possível demonstrar que existe um único operador linear, denotado por $A^{\dag}$ e chamado \emph{operador adjunto} de $A$, que satisfaz a seguinte propriedade:
   \begin{itemize}
    \item[(OA1)] \quad \quad \quad \quad \quad \quad \quad  $\big( \ket{\phi} , A\ket{\psi} \big) = \big( A^{\dag}\ket{\phi} , \ket{\psi} \big) $.
   \end{itemize}
   
   Para qualquer base $\beta$, a matriz do operador $A^{\dag}$ está relacionada com a matriz de $A$ por
   \begin{itemize}
    \item[(OA2)] \quad \quad \quad \quad \quad \quad \quad \quad $ [A^\dag ]_\beta = [A]_\beta^{\ \dag} = \big( [A]_\beta^{\ *}\big)^{T} \ , $
    \end{itemize}
   isto é, a matriz $[A]_\beta^{\ \dag}$ é obtida de $[A]_\beta$ conjugando suas entradas e tomando a transposta. 
   
   Algumas propriedades da adjunta estão dispostas na seguinte lista:
   \begin{itemize}
    \item $ \big( A^\dag \big)^\dag = A $ \ \ (Involução)
    \item $ \displaystyle \left( \sum_k a_k A_k \right)^{\dag} = \sum_k a_k^{\ *} A_k^{\ \dag}$ \ \ (Antilinearidade)
    \end{itemize}

  \begin{example}
   A matriz adjunta de 
   \[ A = \mqty[ 2 & 1+i \\ -1 & 5i ] \]
   é a matriz conjugada transposta
   \[ A^\dag = \mqty[ 2 & -1 \\ 1-i & -5i] \ . \]
  \end{example}
  
 \end{subsection}
 
  \begin{subsection}{Operadores Normais}
   Um operador é dito \emph{normal} se comuta com seu operador adjunto:
    \begin{itemize}
    \item[(ON)] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   $ A \cdot A^{\dag} = A^{\dag} \cdot A $.
   \end{itemize}
   \begin{example}
    O operador definido pela matriz
    \[ A = \mqty[ 1+i & -1 \\ 1 & 1-i ]   \]
    é normal, pois satisfaz $A A^\dag = A^\dag A$. De fato,
    \[ A^\dag = \mqty[ 1-i & 1 \\ -1 & 1+i ] \]
    e tem-se que
    \[ A A^\dag = \mqty[ 1+i & -1 \\ 1 & 1-i ] \mqty[ 1-i & 1 \\ -1 & 1+i ] = \mqty[ 3 & 0 \\ 0 & 3 ] \]
    \[ A^\dag A = \mqty[ 1-i & 1 \\ -1 & 1+i ] \mqty[ 1+i & -1 \\ 1 & 1-i ] = \mqty[3 & 0 \\ 0 & 3 ] \]

   \end{example}

   
   A importância do operador normal decorre do seguinte teorema:
   \begin{theorem}[Teorema Espectral]\label{cap2:teorema_espectral_op_normal}
    Um operador é normal se, e somente se, for diagonalizável.
   \end{theorem}
   Dessa forma, basta checar se um operador é normal para se saber se ele admite uma base de autovetores e uma representação por matriz diagonal. 
   
  \end{subsection}

  \begin{subsection}{Operadores Hermitianos ou Autoadjuntos}\label{cap2:op_hermitianos}
  
    Um operador $H$ é dito \emph{hermitiano}, ou \emph{autoadjunto} se valer a seguinte propriedade:
    \begin{itemize}
    \item[(OH)] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \quad   $ H^{\dag} =  H $.
   \end{itemize}
   
   \begin{example}
    O operador dado por
    \[ A = \mqty[1 & -i \\ i & 1] \]
    é autoadjunto, pois
    \[ A^\dag = \mqty[1 & -i \\ i & 1] = A \ . \]
   \end{example}
   
   Os operadores hermitianos estão relacionados com a evolução no tempo de um sistema quântico fechado e com a medida de um observável.  
   
   Um operador hermitiano é, automaticamente, um operador normal, tendo em vista que $H H^\dag = H^2 = H^\dag H $. Conforme o teorema \ref{cap2:teorema_espectral_op_normal}, todo operador hermitiano é, pois, diagonalizável. Além disso, há um teorema que permite tirar conclusões a respeito dos autovalores de uma matriz hermitiana.
   
   \begin{theorem}[Teorema Espectral para Matrizes Hermitianas]\label{cap2:teorema_espectral_op_hermitiano}
       Um operador normal é hermitiano se, e somente se, todos os seus autovalores são reais. 
   \end{theorem}
   
   
  \end{subsection}
  
  \begin{subsection}{Operadores Unitários}\label{cap2:operadores_unitarios}
   Um operador $U$ é dito \emph{unitário} se satisfizer alguma das condições equivalentes:
       \begin{itemize}
    \item[(OU1)] $ U^{\dag} =  U^{-1} $.
    \item[(OU2)] As linhas ou as colunas de $[U]_\beta$ são vetores ortonormais em $\mathbb{C}^n$, para alguma base $\beta$.
    \item[(OU3)] $U$ é uma isometria, isto é, preserva o produto interno entre vetores (e em consequência, preserva também a distância entre vetores): $ \big( U\ket{\phi} , U\ket{\psi} \big) = \big( \ket{\phi} , \ket{\psi} \big) $.
   \end{itemize}
   
   \begin{example}\label{cap2:ex_operador_unitario}
    O operador definido pela matriz
    \[ H = \frac{1}{\sqrt{2}}\mqty[1 & \phantom{-}1 \\ 1 & -1 ] \]
    é unitário, pois as colunas $\ket{c_0} = \frac{1}{\sqrt{2}}\mqty[ 1 \\ 1]$ e $\ket{c_1} = \frac{1}{\sqrt{2}} \mqty[\phantom{-}1 \\ -1 ]$ são vetores ortonormais:
    \begin{eqnarray*}
     \norm{\ket{c_0}} &=& \norm{ \frac{1}{\sqrt{2}}\mqty[ 1 \\ 1]} = \frac{1}{\sqrt{2}} \sqrt{1^2 + 1^2} = 1 \\
     \norm{\ket{c_0}} &=& \norm{ \frac{1}{\sqrt{2}}\mqty[ \phantom{-}1 \\ -1]} = \frac{1}{\sqrt{2}} \sqrt{1^2 + (-1)^2} = 1 \\
     \braket{c_0}{c_1} &=& \mqty[ 1 & 1 ] \mqty[\phantom{-}1 \\ -1 ] = 1 - 1 = 0 
    \end{eqnarray*}
    
   \end{example}

   O produto de dois operadores unitários é unitário: 
   \[ (U_1 U_2)^\dag = U_2^\dag U_1^\dag = U_2^{-1} U_1^{-1} = (U_1 U_2)^{-1} \ . \]
   
   A condição de um operador ser unitário também implica normalidade, visto que $U U^\dag = I = U^\dag U$. De acordo com o teorema \ref{cap2:teorema_espectral_op_normal}, todo operador unitário é, então, diagonalizável. Pode-se mostrar o seguinte teorema.
   
   \begin{theorem}[Teorema Espectral para Operadores Unitários]\label{cap2:teorema_espectral_op_unitário}
    Seja $U$ uma matriz normal. $U$ é uma matriz unitária se, e somente se, os seus autovalores são números complexos de módulo 1, logo exprimíveis na forma $\lambda = e^{i\theta}$ para algum $\theta \in \mathbb{R}$. 
   \end{theorem}
   
   
  \end{subsection}

  \begin{subsection}{Operadores Positivos}
   Um operador é dito \emph{positivo} quando satisfaz a seguinte propriedade:
   \begin{itemize}
    \item[(OPos)] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  $ \mel{\psi}{P}{\psi} \geq 0 \ , \ \ \forall \ket{\psi}$.
   \end{itemize}
   
   É possível demonstrar que um operador positivo é, automaticamente, hermitiano, e, portanto, todos os seus autovalores são reais. Além disso, a propriedade (OPos) é equivalente a dizer que todos os autovetores de $P$ são números reais não-negativos $\lambda \geq 0$. 
   
   Diz-se que um operador é \emph{positivo definido} quando satisfaz a condição seguinte, mais rigorosas que (OPos):
    \begin{itemize}
    \item[(OPosDef)] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  $ \mel{\psi}{P}{\psi} > 0 \ , \ \ \forall \ket{\psi}\neq 0$
   \end{itemize}
   Essa condição é equivalente a afirmar que todos os autovalores de $P$ são números reais positivos $\lambda >0$.

   \begin{example}
    O operador definido pela matriz
    \[ A = \mqty[ 1 & -2 \\ 0 & 2 ] \] 
    é positivo. De fato, por se tratar de uma matriz triangular, os autovalores podem ser obtidos diretamente da diagonal: $\lambda = 1$ e $\lambda = 2$. Seus autovalores são todos não negativos, do que decorre que $A$ é positivo. Como nenhum autovalor é nulo, o operador é também positivo definido. 
    
   \end{example}

   \end{subsection}

  \begin{subsection}{Operadores de Projeção}
   Um \emph{operador de projeção} é um operador $P$ que satisfaz:
   \begin{itemize}
    \item[(OProj)] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \quad   $ P^2 =  P $.
   \end{itemize}
   
   Os autovalores de $P$ podem assumir os valores $\lambda = 0$ ou $\lambda = 1$. De fato, se $\lambda$ é um autovalor com autovetor $\ket{v}\neq 0$ associado, tem-se $P\ket{v} = \lambda \ket{v} \implies \lambda \ket{v} = P\ket{v} = PP\ket{v} = \lambda P\ket{v} = \lambda^2 \ket{v}$, logo, $(\lambda^2 - \lambda)\ket{v} = 0$ e, como $\ket{v} \neq 0$, tem-se $\lambda^2 - \lambda = 0$ e, por fim, $\lambda = 0$ ou $\lambda = 1$. Isso prova, em virtude dos teoremas \ref{cap2:teorema_espectral_op_hermitiano} e \ref{cap2:teorema_espectral_op_unitário}, que todo operador de projeção é hermitiano e positivo. 
   
   Considere o subspaço vetorial de dimensão finita $W = P(V)$ (imagem de $P$). Seja $k=\dim W$ e tome uma base ortonormal $\big\{ \ket{v_0}, \ldots , \ket{v_{k-1}} \big\}$ de $P(V)$. Estendendo-se a $\big\{ \ket{v_0}, \ldots , \ket{v_{k-1}}, \ket{v_k} , \ldots , \ket{v_{n-1}} \big\}$ base ortonormal do espaço $V$, é possível escrever o operador $P$ como
   \[ P = \text{Proj}_W = \sum_{j=0}^{k-1} \op{v_j}{v_j} \ .\] 
   O operador 
   \[ Q = I - P = \text{Proj}_{W^\perp} =  \sum_{j=k}^{n-1} \op{v_j}{v_j} \]
   é um operador de projeção no subespaço $W^{\perp}$. 
   
   Tem-se que $\text{Proj}_W + \text{Proj}_{W^\perp} = I$ e portanto, todo vetor $\ket{\psi}$ pode ser decomposto na soma de projeções em $W$ e em $W^{\perp}$:
   \[ \ket{\psi} = \underbrace{\text{Proj}_W(\ket{\psi})}_{\in W} + \underbrace{\text{Proj}_{W^\perp}(\ket{\psi})}_{\in W^{\perp}} \ .\] 
   
   \begin{example}
    Os operadores no espaço de estados de 1 qubit
    \begin{eqnarray*}
     \op{0}{0} &=& \mqty[1 & 0 \\ 0 & 0 ] \\
     \op{1}{1} &=& \mqty[0 & 0 \\ 0 & 1 ]  \ ,
    \end{eqnarray*}
    são as projeções na direção dos vetores $\ket{0}$ e $\ket{1}$, respectivamente. Tem-se que 
    \[ I =  \op{0}{0} + \op{1}{1} \ , \]
    que é a relação de completude.
    
    Se denotarmos por $W = \text{span}\{\ket{0} \}$ o subespaço gerado por $\ket{0}$, tem-se que $W^\perp = \text{span}\{ \ket{1} \}$, $\op{0}{0} = \text{Proj}_W$, $\op{1}{1} = I - \op{0}{0} = \text{Proj}_{W^\perp} $ e qualquer vetor $\ket{\psi}$ pode ser escrito como
    \begin{eqnarray*}
      \ket{\psi} 
      &=& I \ket{\psi} = (\op{0}{0} + \op{1}{1}) \ket{\psi} \\
      &=& \ \op{0}{0} \ket{\psi} \ + \  \op{1}{1}\ket{\psi} \\
      &=& \text{Proj}_W \ket{\psi} + \text{Proj}_{W^\perp} \ket{\psi} \ . 
    \end{eqnarray*}
    Se $\ket{\psi} = a \ket{0} + b \ket{1}$, então
    \begin{eqnarray*}
     \text{Proj}_W\ket{\psi} 
     &=& \op{0}{0}(a \ket{0} + b \ket{1}) = \ket{0}a\underbrace{\braket{0}{0}}_{=1} + \ket{0}b\underbrace{\braket{0}{1}}_{=0} = a \ket{0} \\
     \text{Proj}_{W^\perp}\ket{\psi} 
     &=& \op{1}{1}(a \ket{0} + b \ket{1}) = \ket{1}a\underbrace{\braket{1}{0}}_{=0} + \ket{1}b\underbrace{\braket{1}{1}}_{=1} = b \ket{1} \ . \\
    \end{eqnarray*}
  
   \end{example}

   
  \end{subsection}
  
  \begin{subsection}{Resumo}
   A figura abaixo traz um quadro resumo dos operadores especiais abordados neste capítulo. 
   
\begin{figure}[H]
\centering
\includegraphics[scale=0.07]{cap2/figures/operators.png}
\caption{Relação entre os operadores especiais estudados neste capítulo. Fonte: Slides de Álgebra Linear, UFSM \cite{slides:al_jonas}]}
\end{figure}

\begin{table}[H]
 \centering
 \begin{tabular}{l|c}
  Operador & Propriedade   \bigstrut \\ \hline
  Normal  &   $N N^\dag = N^\dag N$  \bigstrut \\
  %\!\!\!\!\! \begin{tabular}{l} Autoadjunto \\ ou Hermitiano \end{tabular} 
  Autoadjunto ou Hermitiano& $H^\dag = H$  \bigstrut \\
  Unitário & $U^{-1} = U^\dag$ \bigstrut \\
  Projetor & $P^2 = P$ \bigstrut \\ 
  Positivo & $\bra{\psi}P\ket{\psi} \geq 0 \ , \ \ \forall \ket{\psi}$ \bigstrut \\
 % \begin{tabular}{l} Positivo \\ definido \end{tabular} 
  Positivo definido& $\bra{\psi}P\ket{\psi} > 0 \ , \ \ \forall \ket{\psi}\neq 0$ \bigstrut \\
 \end{tabular}
 \caption{Resumo das propriedades dos operadores especiais estudados neste capítulo.}
\end{table}


 \end{subsection}

\end{section}
 
 
 \begin{section}{Produto Tensorial}
  É possível compor dois espaços vetoriais para formar um terceiro espaço. Uma maneira de fazer isso é por meio do \emph{produto tensorial}.Em Computação Quântica, essa construção é fundamental para se trabalhar com sistemas compostos por mais de um qubit. Um sistema de dois qubits será o produto tensorial de dois espaços $\mathbb{C}^2$, que modelam um qubit. 
  
  \begin{subsection}{\nohyphens{Espaço Vetorial do Produto Tensorial}}
   Dados dois espaços vetorias $V$ e $W$, com bases $\beta_V = \{ \ket{v_k} \}$ e $\beta_W = \{ \ket{w_l} \}$, o \emph{produto tensorial} de $V$ e $W$, denotado por $V\otimes W$, é definido como o espaço vetorial gerado pela base:
   \[ v_k \otimes w_l \ , \ \ \begin{matrix} k=0 , \ldots , \dim V - 1 \\  l=0 , \ldots , \dim W - 1 \end{matrix} \ . \]
   
   A dimensão do espaço vetorial do produto tensorial é, portanto, 
   \[ \dim V\otimes W = \dim V \cdot \dim W \ . \]
   
   O produto tensorial $\otimes$ forma uma dupla ordenada com propriedades diferentes das do produto cartesiano. Essas propriedades, listadas abaixos, são chamadas conjuntamente de \emph{bilinearidade}:
   \begin{itemize}
    \item Para todos $z \in \mathbb{C}$, $\ket{v}\in V$ e $\ket{w} \in W$, 
    \[ z \cdot (\ket{v} \otimes \ket{w}) = (z\ket{v}) \otimes \ket{w} = \ket{v} \otimes(z\ket{w}) \ . \]
    \item Para todos  $\ket{v^1},\ket{v^2}\in V$ e $\ket{w} \in W$, 
    \[ (\ket{v^1}+ \ket{v^2}) \otimes \ket{w} = \ket{v^1} \otimes \ket{w} + \ket{v^2} \otimes \ket{w} \ . \]
    \item Para todos $\ket{v}\in V$ e $\ket{w^1},\ket{w^2} \in W$, 
    \[ \ket{v} \otimes (\ket{w^1} + \ket{w^2}) = \ket{v} \otimes \ket{w^1} + \ket{v} \otimes \ket{w^2} \ . \]
   \end{itemize}

   Um elemento genérico do espaço $V\otimes W$ é uma combinação linear dos vetores da base $\ket{v_k} \otimes \ket{w_l}$. Em geral, essa combinação não pode ser escrita da forma fatorada $\ket{v} \otimes \ket{w}$. 
   
   \begin{example}\label{cap2:ex_espaco_vet_2qubits}
    O sistema composto por 2 qubits é dado pelo produto tensorial de dois espaços vetoriais de 1 qubit (isto será visto com mais detalhes na seção \ref{cap3:sec_sistemas_compostos}). Esse espaço vetorial é denotado por $\mathbb{C}^2 \otimes \mathbb{C}^2$. A base desse espaço é formada pelos 4 vetores
    \begin{eqnarray*}
     \ket{00} &=& \ket{0} \ket{0} = \ket{0} \otimes \ket{0} \\
     \ket{01} &=& \ket{0} \ket{1} = \ket{0} \otimes \ket{1} \\
     \ket{10} &=& \ket{1} \ket{0} = \ket{1} \otimes \ket{0} \\
     \ket{11} &=& \ket{1} \ket{1} = \ket{1} \otimes \ket{1} \ ,
    \end{eqnarray*}
     e as igualdades apresentadas acima apenas referem-se a notações alternativas e mais compactas. A ordem em que as entradas aparecem no produto tensorial é importante, de forma que $\ket{01} \neq \ket{10}$, por exemplo. 
   
   Um vetor qualquer $\ket{\psi} \in \mathbb{C}^2 \otimes \mathbb{C}^2$ pode ser escrito como
   \[ \ket{\psi} = a \ket{00} + b\ket{01} + c \ket{10} + d\ket{11} \ , \] 
   com $a,b,c,d \in \mathbb{C}$. Alguns exemplos de vetores pertencentes ao espaço em questão são:
   \[ \frac{\ket{00} + \ket{11}}{\sqrt{2}} \]
   \[ \ket{0} \otimes \left(\ket{0} + 2\ket{1} \right) = \ket{00} + 2 \ket{01} \]
   \[ \left( 5\ket{1}\right)   \ket{1}  =  5 \ket{11} \]
   \[ \left(\frac{1}{\sqrt{2}}\ket{0} - \frac{i}{\sqrt{2}} \right) \ket{0} = \frac{1}{\sqrt{2}}\ket{00} - \frac{i}{\sqrt{2}}\ket{10} \ . \]
   As igualdades acima são exemplos da bilinearidade do produto tensorial. 
   \end{example}

   
  \end{subsection}
  
  \begin{subsection}{\nohyphens{Comparação do Produto Tensorial com o \\Produto Cartesiano}}
   O \emph{produto cartesiano}, às vezes chamado \emph{soma direta} é outra maneira de se compor dois espaços vetoriais em um espaço ``maior'', denotado por $V \times W$ ou por $V \oplus W$. O produto cartesiano é formado por duplas $\big( \ket{v} , \ket{w} \big)$, com $\ket{v} \in V$ e $\ket{w} \in W$. Nesta subseção, a notação $( \cdot \, , \cdot )$ refere-se a par ordenado em vez de produto interno como no restante do texto.
   
   As diferenças entre essas duas operações estão dispostas no que segue:
   \begin{table}
   \centering
   \small
     \begin{tabular}{l|cc}
      & Produto Tensorial & Produto Cartesiano  \\
      & & ou Soma Direta \\ \hline
      \bigstrut Notação: &  $V \otimes W$ & $V \times W = V \oplus W$ \\
      \bigstrut Base: & $\ket{v_k} \otimes \ket{w_l} $  &   $ \big( \ket{v_k} , 0 \big) \ , \ \big( 0 , \ket{w_l} \big) $ \\
      \bigstrut Dimensão: & $ \dim V \otimes W = \dim V \cdot \dim W $ &   $ \dim V \oplus W = \dim V + \dim W $
   \end{tabular}
   \normalsize
   \caption{Tabela comparativa entre produto tensorial e produto cartesiano (também chamado soma direta).}
   \end{table}
   Outra diferença é que a soma, no produto cartesiano, é uma soma entrada a entrada
   \[ \big( \ket{v^1} , \ket{w^1} \big) + \big( \ket{v^2} , \ket{w^2} \big) = \big( \ket{v^1} + \ket{v^2} , \ket{w^1} + \ket{w^2} \big) \ , \]
   enquanto que a soma no produto tensorial, de modo geral, não se reduz
   \[ \ket{v^1} \otimes \ket{w^1}  +  \ket{v^2} \otimes \ket{w^2}  \ , \]
   a não ser que, por exemplo, $\ket{v^1} = \ket{v^2}$, de modo que
   \[ \ket{v^1} \otimes \ket{w^1}  +  \ket{v^1} \otimes \ket{w^2} = \ket{v^1} \otimes \big( \ket{w^1}  + \ket{w^2} \big) \ . \]
   A multiplicação por escalar no produto cartesiano também é entrada a entrada
   \[ z \cdot \big( \ket{v} , \ket{w} \big) = \big( z\ket{v} , z\ket{w} \big) \ , \]
   enquanto que, no produto tensorial, o escalar pode ser incorporado a qualquer das duas entradas, mas deve ir para apenas uma delas
   \[ z \cdot \big(\ket{v} \otimes \ket{w}\big) = \big(z\ket{v}\big) \otimes \ket{w} = \ket{v} \otimes \big(z\ket{w} \big) \ . \]
   
  \end{subsection}

  \begin{subsection}{Produto Interno}
   Sejam $V$ e $W$ espaços de Hilbert, isto é, espaços munidos de produto interno. O produto tensorial $V\otimes W$ pode ser munido com um produto interno derivado dos produtos internos de $V$ e de $W$. Defina:
   \begin{eqnarray}
     \Big( \ket{v_k}\otimes \ket{w_l} , \ket{v_{k^\prime}}\otimes \ket{w_{l^\prime}} \Big) 
   &=&  \Big( \ket{v_k} , \ket{v_{k^\prime}} \Big)_V \cdot \Big( \ket{w_l} , \ket{w_{l^\prime}} \Big)_W  \nonumber \\
   &=& \braket{v_k}{v_{k^\prime}}  \cdot \braket{w_l}{w_{l^\prime}} =  \delta_{k,k\prime} \delta_{l,l^\prime} \ , \quad \quad \quad    \label{cap2:def_produto_interno_vetores_da_base}
   \end{eqnarray}
   estendendo a definição para elementos arbitrários do produto tensorial por linearidade na segunda entrada e antilinearidade na primeira. 
   
   Para dois vetores da forma $\ket{v^1}\otimes \ket{w^1}$ e $\ket{v^2}\otimes \ket{w^2}$ do produto tensorial, pode-se denotar
   \[  \Big( \ket{v^1}\otimes \ket{w^1} ,   \ket{v^2}\otimes \ket{w^2} \Big) = \big( \bra{v^1}\otimes \bra{w^1} \big) \big(  \ket{v^2}\otimes \ket{w^2} \big) \ , \]
   e decorre da definição de produto interno que
   \[  \big( \bra{v^1}\otimes \bra{w^1} \big) \big(  \ket{v^2}\otimes \ket{w^2} \big) = \braket{v^1}{v^2}_V  \cdot \braket{w^1}{w^2}_W \ . \]
   
   \begin{example}
    O espaço vetorial $\mathbb{C}^2 \otimes \mathbb{C}^2$ que descreve 2 qubits foi apresentado no exemplo \ref{cap2:ex_espaco_vet_2qubits}. O produto interno nesse espaço é explicitado no que segue. 
    Use os índices $A$ e $B$ para fazer referência à primeira e à segunda entrada tensorial, respectivamente.
    
    O produto interno dos vetores da base é dado pela equação \ref{cap2:def_produto_interno_vetores_da_base}, que se traduz em 
    \begin{eqnarray*}
    \braket{jk}{pq}_{AB} 
    &=& \big( \ket{jk} , \ket{pq} \big)_{AB} \\
    &=& \big(\ket{j},\ket{p} \big)_A \cdot \big( \ket{k},\ket{q} \big)_B \\
    &=& \braket{j}{p}_A \cdot \braket{k}{q}_B \\
    &=& \delta_{j,p} \cdot \delta_{k,q}  = \delta_{jk,pq} \ ,
    \end{eqnarray*}
    em que $j,k,p,q = 0,1$. Por exemplo, 
    \[ \text{$\braket{01}{01} = 1$, $\braket{11}{10} = 0$ e $\braket{10}{01} = 0$ .} \]
    
    Sejam 
    \begin{eqnarray*}
        \ket{\phi} &=& a_1 \ket{00} + b_1 \ket{01} + c_1 \ket{10} + d_1 \ket{11} \\
        \ket{\psi} &=& a_2 \ket{00} + b_2 \ket{01} + c_2 \ket{10} + d_2 \ket{11} \ .
    \end{eqnarray*}
    O produto interno de $\ket{\phi}$ com $\ket{\psi}$ é dado por
    \begin{eqnarray*}
     \braket{\phi}{\psi} 
     &\! \! \! \! =&\! \! \! \! \! \big( \text{\small{$a_1^{\ *} \bra{00} + b_1^{\ *} \bra{01} + c_1^{\ *} \bra{10} + d_1^{\ *} \bra{11}$}} \big) \big( \text{\small{$a_2 \ket{00} + b_2 \ket{01} + c_2 \ket{10} + d_2 \ket{11}$}} \big) \\
     &\! \! \! \! =&  a_1^{\ *} a_2 + b_1^{\ *} b_2 + c_1^{\ *} c_2 + d_1^{\ *} d_2 \ . 
    \end{eqnarray*}
    A norma de $\ket{\phi}$ é dada por 
    \[ \norm{\ket{\phi}} = \sqrt{\abs{a_1}^2 + \abs{b_1}^2 + \abs{c_1}^2 + \abs{d_1}^2}\  \ . \]
    
    Exemplos:
    \[ \begin{split}
     & \left( \frac{\bra{0} + \bra{1}}{\sqrt{2}} \right)_{A} \bra{1}_{B}\  \ket{0}_A \left(\frac{\ket{0}-i\ket{1}}{\sqrt{2}} \right)_B \\
    &\ = \ \  \left( \frac{\braket{0}{0} + \braket{1}{0}}{\sqrt{2}} \right)_A \left( \frac{\braket{1}{0} -i \braket{1}{1}}{\sqrt{2}} \right)_B \\
    &\ = \ \  \frac{1+0}{\sqrt{2}} \cdot \frac{0 - i}{\sqrt{2}} = \frac{-i}{2}
    \end{split} \]
   \[ \norm{\ket{01} + i \ket{10}} = \sqrt{\abs{1}^2 + \abs{i}^2} = \sqrt{2} \ . \]
   \end{example}

  \end{subsection}

  \begin{subsection}{Operadores}
   Sejam $A$ um operador em $V$ e $B$ um operador em $W$. É possível definir um operador em $V\otimes W$, denotado por $A\otimes B$ de forma que
   \[ (A \otimes B) (\ket{v} \otimes \ket{w}) = A\ket{v} \otimes B\ket{w} \ . \]
   
   Todo operador em $V \otimes W$ pode ser decomposto em uma combinação linear de operadores da forma apresentada anteriormente. 

   Pode-se mostrar que a composição, ou produto, de dois operadores $A \otimes B$ e $A' \otimes B'$ é dada por:
   \[ (A \otimes B) (A' \otimes B') = AA' \otimes BB' \ . \]
   
   \begin{example}
    Sejam $X$ e $H$ operadores no espaço de 1 qubit dados por 
    \[\begin{array}{ll}
        \begin{matrix}
        X\ket{0} = \ket{1} \\
        X\ket{1} = \ket{0}
       \end{matrix}  
       &
       X = \mqty[ 0 & 1 \\ 1 & 0 ] 
       \\ & \\
        \begin{matrix}
        H\ket{0} = \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt{2}} \ket{1} \\
        H\ket{1} = \frac{1}{\sqrt{2}} \ket{0} - \frac{1}{\sqrt{2}} \ket{1}
       \end{matrix}  
       &
       H = \frac{1}{\sqrt{2}} \mqty[ 1 & 1 \\ 1 & -1 ] \ .
      \end{array} \]
    Esses operadores foram utilizados em vários dentre os exemplos anteriores.
    
    O operador $H \otimes X$ atua no espaço de 2 qubits como no exemplo abaixo.
    \[ \begin{split}
         &H\otimes X \big(\ket{0} \otimes (\ket{0} + i \ket{1} ) \big) \\
         &\ \ = H\ket{0} \otimes \big(X(\ket{0} + i \ket{1} ) \big) \\
         &\ \ = \left(\frac{\ket{0} + \ket{1}}{\sqrt{2}} \right) \otimes \big( \ket{1} + i \ket{0} \big) \\
         &\ \ = \frac{i}{\sqrt{2}}\ket{00} + \frac{1}{\sqrt{2}}\ket{01} + \frac{i}{\sqrt{2}}\ket{10} + \frac{1}{\sqrt{2}}\ket{11} \ . 
       \end{split} \]
   Se forem usados rótulos $1$ e $2$ para as entradas tensoriais, a conta do exemplo acima poderia ser reescrita como
   \[ \begin{split}
         &H_1 X_2 \ket{0}_1  (\ket{0} + i \ket{1} )_2  \\
         &\ \ = H_1 \ket{0}_1  X_2(\ket{0} + i \ket{1} )_2 \\
         &\ \ = \left(\frac{\ket{0}_1 + \ket{1}_1}{\sqrt{2}} \right) \big( \ket{1}_2 + i \ket{0}_2 \big) \\
         &\ \ = \frac{i}{\sqrt{2}}\ket{00}_{12} + \frac{1}{\sqrt{2}}\ket{01}_{12} + \frac{i}{\sqrt{2}}\ket{10}_{12} + \frac{1}{\sqrt{2}}\ket{11}_{12} \ . 
       \end{split} \]
   
   Mais detalhes sobre a notação encontram-se na seção \ref{cap2:produto_tensorial_notacao}
   \end{example}

  \end{subsection}
  
  \begin{subsection}{Produto de Kronecker}
   Até então, o conceito de produto tensorial foi apresentado de maneira abstrata. É possível abordar esse conceito de maneira matricial também. Fixadas bases para $V$ e $W$, os vetores $\ket{v}$ e $\ket{w}$ desses espaços podem ser representados como vetores coluna. 
   O vetor $\ket{v} \otimes \ket{w}$ também pode ser representado como vetor coluna fazendo-se o \emph{produto de Kronecker}. 
   
   \[ \ket{v} \otimes \ket{w} = \mqty[ a_0 \\ a_1 \\ \vdots \\ a_{n-1} ] \otimes \mqty[ b_0 \\ b_1 \\ \vdots \\ b_{m-1} ] =  \mqty[ a_0 \ket{w} \\ a_1 \ket{w} \\ \vdots \\ a_{n-1} \ket{w} ]  = \mqty[ a_0 b_0 \\ a_0 b_1 \\ \vdots \\ a_0 b_{m-1} \\ a_1 b_0 \\ a_1 b_1 \\ \vdots \\ a_{n-1} b_{m-1} ] \]
   
   Essa operação é mais facilmente compreendida por meio de um exemplo.
   
   \begin{example}
   \[ \mqty[ 1 \\ 2 ] \otimes \mqty[ 3 \\ 4 \\ 5 ] = \mqty[ 1 \mqty[ 3 \\ 4 \\ 5 ] \\ \\ 2 \mqty[ 3 \\ 4 \\ 5 ] ] = \mqty[ 3 \\ 4 \\ 5 \\ 6 \\ 8 \\ 10 ] \]
   \end{example}
   
   O produto tensorial de operadores em $V$ e $W$ também pode ser obtido matricialmente por meio do produto de Kronecker.
   
   \[ A \otimes B = \mqty[ a_{0,0} & \cdots & a_{0,n-1} \\ a_{1,1} & & a_{1,n-1} \\ \vdots & & \vdots \\ a_{n-1,0} & \cdots & a_{n-1,n-1} ]  \otimes B = \mqty[ a_{0,0}B & \cdots & a_{0,n-1}B \\ a_{1,1}B & & a_{1,n-1}B \\ \vdots & & \vdots \\ a_{n-1,0}B & \cdots & a_{n-1,n-1}B ] \]
   
   \begin{example}
   \[ \mqty[ 0 & 1 \\ 1 & 0 ] \otimes \mqty[ 1 & 2 \\ 3 & 4 ] = \mqty[ 0 \mqty[ 1 & 2 \\ 3 & 4 ] & 1 \mqty[ 1 & 2 \\ 3 & 4 ] \\ \\ 1 \mqty[ 1 & 2 \\ 3 & 4 ]  & 0 \mqty[ 1 & 2 \\ 3 & 4 ]  ] = \mqty[ 0 & 0 & 1 & 2 \\ 0 & 0 & 3 & 4 \\ 1 & 2 & 0 & 0 \\ 3 & 4 & 0 & 0 ]  \]
   \end{example}
   
   O produto de Kronecker, em geral, não é comutativo. 
   
   Se $A \in M(m_A,n_A)$ e $B \in M(m_B, n_B)$, então a matriz $A \otimes B$ tem $m_Am_B$ linhas e $n_An_B$ colunas, ou seja, $A\otimes B \in M(m_Am_B,n_An_B)$. Uma forma geral de escrever o elemento $j,k$ da matriz $A\otimes B$ é
   \[ ( A\otimes B )_{j,k} = a_{\text{quoc}(j,m_B),\text{quoc}(k,n_B)}\  b_{\text{resto}(j,m_B),\text{resto}(k,n_B)} \ \ , \]
   em que
   \[ \begin{split}
       j &= 0 , 1 , \ldots , m_A m_B - 1 \\
       k &= 0 , 1 , \ldots , n_A n_B - 1 
      \end{split} \]
   e, para $x,y$ inteiros,
   \[ \begin{split}
       \text{quoc}(x,y) \ \ &\text{é o quociente da divisão $x / y$} \\
       \text{resto}(x,y) \ \ &\text{é o resto da divisão $x / y$} \ .
      \end{split} \]
      
    \begin{example}
     $A_{2\times2}$ , $B_{3\times2}$. O elemento $4,2$ da matriz $A\otimes B$ pode ser obtido por:
     \[ (A \otimes B)_{4,2} = a_{\text{quoc}(4,3),\text{quoc}(2,2)}\  b_{\text{resto}(4,3),\text{resto}(2,2)} = a_{1,1} b_{1,0} \ . \]
     A matriz $A\otimes B$ está representada abaixo, destacando-se o elemento $4,2$:
     \[ A \otimes B = \mqty[ a_{0,0} B & a_{0,1} B \\ a_{1,0} B & a_{1,1} B ] =   \mqty[ a_{0,0} \mqty[ b_{0,0} & b_{0,1} \\ b_{1,0} & b_{1,1} \\ b_{2,0} & b_{2,1} ] & a_{0,1} \mqty[ b_{0,0} & b_{0,1} \\ b_{1,0} & b_{1,1} \\ b_{2,0} & b_{2,1} ] \\ & \\ a_{1,0} \mqty[ b_{0,0} & b_{0,1} \\ b_{1,0} & b_{1,1} \\ b_{2,0} & b_{2,1} ] & \boxed{a_{1,1}} \mqty[ b_{0,0} & b_{0,1} \\ \boxed{b_{1,0}} & b_{1,1} \\ b_{2,0} & b_{2,1} ] ]   \ . \]
    \end{example}


  \end{subsection}

  \begin{subsection}{\nohyphens{Produto Tensorial de Vários Espaços Vetoriais}}
   O produto tensorial de vários espaços vetoriais segue a mesma ideia do caso de dois espaços, apresentado anteriormente. A dimensão será o produto das dimensões de cada espaço. O produto tensorial é associativo, então não é necessário usar parênteses num produto tensorial de vários espaços. Não é possível comutar os fatores do produto tensorial.    
  \end{subsection}

  \begin{subsection}{Notação}\label{cap2:produto_tensorial_notacao}
   Em situações práticas, costuma-se usar índices em cada fator do produto tensorial para evitar confusões. Também há variantes que deixam a notação mais curta. Alguns comentários a respeito dessas variantes são abordados nesta seção. Para tratar disso, considere um exemplo em $V\otimes W \otimes U$.
  
  % ocultando símbolo otimes nos kets
  Pode-se denotar um elemento da forma $\ket{v} \otimes \ket{w} \otimes \ket{u} $ omitindo-se o símbolo $\otimes$: $\ket{v}  \ket{w}  \ket{u} $. Nesse caso, deve-se tomar cuidado para não confundir essa justaposição com o produto matricial; nas situações de interesse, normalmente o produto matricial não será possível e há menos risco de confusão. 
   
  % índices nos kets
  Costuma-se atribuir índices aos fatores: $\ket{v} \otimes \ket{w} \otimes \ket{u} = \ket{v}_V \otimes \ket{w}_W \otimes \ket{u}_U  =  \ket{v}_V \ket{w}_W  \ket{u}_U$. Ainda, pode-se escrever esse elemento como: $\ket{v}_1 \ket{w}_2 \ket{u}_3 $, ou $\ket{v \, w \, u}_{VWU}$, ou, ainda, qualquer notação equivalente, que evidencie a que espaço cada ket pertence. Com a identificação por índices, é possível até trocar a ordem em que são escritos; não deve ser confundido com a comutatividade dos fatores, que não é permitida em geral. Assim, pode-se escrever: $ \ket{u}_3 \ket{w}_2 \ket{v}_1 $.
  
  % índices nos bras
  Os bras também podem ser rotulados com índices. Por exemplo, escreve-se $\big( \ket{v}_1 \ket{w}_2 \ket{u}_3\big)^\dag =  {}_1\bra{v} {}_2\bra{w} {}_3\bra{u}$, ou $\ket{v \, w \, u}_{VWU}^{\ \ \dag} = {}_{VWU}\bra{v \, w \, u} $.
  
  %operadores
  Considere, agora, os operadores da forma $A\otimes B\otimes C$, com $A$, $B$ e $C$ operadores nos espaços $V$, $W$ e $U$, respectivamente. É possível também atribuir índices nos operadores para lembrar em que espaço cada um deles atua. Por exemplo, pode-se denotar: $A_1\otimes B_2\otimes C_3$. 
  
  Há situações em que se deseja operar apenas em uma das entradas do produto tensorial. Assim, por exemplo, $A\ket{v} \otimes \ket{w}\otimes \ket{u} = A_1 \big( \ket{v} \otimes \ket{w}\otimes \ket{u} \big) $. Formalmente, $A_1 = A \otimes I \otimes I$, nesse caso. O produto (ou composição) dos operadores $A_1 B_2 C_3$ significa $ (A\otimes I \otimes I) (I \otimes B \otimes I)(I \otimes I \otimes C) = AII \otimes IBI \otimes IIC = A\otimes B \otimes C$. Dessa forma, com os índices, pode-se escrever $A_1 B_2 C_3 = A \otimes B \otimes C$ sem perigo de confusão com o produto matricial de $A$, $B$ e $C$; Pode-se até mesmo trocar a ordem de como são escritos: 
  $B_2 A_1 C_3 = A\otimes B \otimes C$.
  
  Outra notação bastante utilizada é quando se deseja realizar o produto tensorial entre $n$ cópias de um vetor $\ket{\psi}$. Define-se 
  \[ \ket{\psi}^{\otimes n} = \underbrace{\ket{\psi} \otimes \ket{\psi}}_{\text{$n$ vezes}} \ . \]
  Essa notação pode ser utilizada para bras e para operadores: $\bra{\psi}^{\otimes n}$, $A^{\otimes n }$. Também é possível usar uma notação análoga ao produtório para denotar, por exemplo, 
  \[ \bigotimes_{i=1}^{n} A_i \ket{0}_i = A_1 \ket{0}_1 \otimes \ldots \otimes A_n \ket{0}_n = A_1 \ldots A_n \ket{0\ldots 0} \ . \]
  
  Há, pois, diversas maneiras de se denotar os mesmos vetores ou operadores. Essa variedade é útil para permitir a escrita de expressões compactas em diversas situações em que o produto tensorial aparece. 
  \end{subsection}
  
  
  
  
 \end{section}